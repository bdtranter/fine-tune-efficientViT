{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdtranter/fine-tune-efficientViT/blob/main/Tuned_EfficientViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We1J9dIdCr13",
        "outputId": "dd56f289-1124-4e1e-deb4-cdfa4ad85735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mit-han-lab/efficientvit.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwvJPix1XeFv",
        "outputId": "a74c0804-3ed6-4bfd-c714-4220b9caf703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'efficientvit'...\n",
            "remote: Enumerating objects: 1524, done.\u001b[K\n",
            "remote: Counting objects: 100% (409/409), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 1524 (delta 299), reused 267 (delta 267), pack-reused 1115 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1524/1524), 217.43 MiB | 16.22 MiB/s, done.\n",
            "Resolving deltas: 100% (764/764), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/efficientvit\")"
      ],
      "metadata": {
        "id": "BnrJFVCobkAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install timm einops opencv-python tqdm onnx onnxsim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1hy16Tybk5_",
        "outputId": "61383926-0614-42df-cb21-2d38f538860f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for onnxsim (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import einops\n",
        "import cv2\n",
        "import tqdm\n",
        "print(\"All imports successful ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUyWE5vsb62I",
        "outputId": "cfdd51a1-0e89-4a1b-999c-f1326f20eba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports successful ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install git+https://github.com/facebookresearch/segment-anything.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyFnavMHfUWo",
        "outputId": "19f5b249-bb56-4890-ba6e-0fb31d73fb7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wHAkvrB4gnJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip once\n",
        "#\"-n\" tells unzip to skip existing files so extraction can safely resume without overwriting work already done.\n",
        "#This enables you to stop and resume the zip process\n",
        "!unzip -n \"/content/drive/MyDrive/datasets/gtFine_trainvaltest.zip\" -d \"/content/drive/MyDrive/datasets/\"\n",
        "!unzip -n \"/content/drive/MyDrive/datasets/leftImg8bit_trainvaltest.zip\" -d \"/content/drive/MyDrive/datasets/\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te5c6hwiDS0A",
        "outputId": "e7481617-f89a-4480-8a51-a33224cb2f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/datasets/leftImg8bit_trainvaltest.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "fQB8xaiS0aq2"
      },
      "outputs": [],
      "source": [
        "#%%writefile cityscapes_for_sam.py\n",
        "# cityscapes_for_sam.py\n",
        "\"\"\"\n",
        "Cityscapes wrapper that produces:\n",
        "    - image tensor\n",
        "    - one binary mask\n",
        "    - one positive point prompt\n",
        "for training a SAM-style model.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import random\n",
        "from typing import Dict, Any, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets import Cityscapes\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CityscapesForSAM(Dataset):\n",
        "    \"\"\"\n",
        "    Wraps torchvision Cityscapes and returns:\n",
        "        image: (3,H,W) float [0,1]\n",
        "        gt_mask: (H,W) float {0,1}\n",
        "        point_coords: (1,2) float [[x, y]] in pixel coords\n",
        "        point_labels: (1,) long [1]  (positive)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root: str, split: str = \"train\"):\n",
        "        super().__init__()\n",
        "        self.base = Cityscapes(\n",
        "            root=root,\n",
        "            split=split,\n",
        "            mode=\"fine\",\n",
        "            target_type=\"semantic\",\n",
        "        )\n",
        "        self.img_transform = T.ToTensor()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.base)\n",
        "\n",
        "    def _sample_prompt_from_mask(\n",
        "        self, sem_mask: Tensor\n",
        "    ) -> Optional[Tuple[Tensor, Tensor, Tensor]]:\n",
        "        \"\"\"\n",
        "        sem_mask: (H,W) long semantic labels.\n",
        "\n",
        "        Returns:\n",
        "            binary_mask: (H,W) float {0,1}\n",
        "            point_coords: (1,2) float [[x, y]]\n",
        "            point_labels: (1,) long [1]\n",
        "        or None if we fail to sample.\n",
        "        \"\"\"\n",
        "        # Remove background / ignore labels as needed.\n",
        "        classes = torch.unique(sem_mask)\n",
        "        classes = classes[(classes != 0) & (classes != 255)]\n",
        "        if len(classes) == 0:\n",
        "            return None\n",
        "\n",
        "        cls = classes[torch.randint(len(classes), (1,))]\n",
        "        region = (sem_mask == cls)\n",
        "        ys, xs = region.nonzero(as_tuple=True)\n",
        "        if len(xs) == 0:\n",
        "            return None\n",
        "\n",
        "        idx = torch.randint(len(xs), (1,))\n",
        "        y, x = ys[idx], xs[idx]\n",
        "\n",
        "        binary_mask = region.float()\n",
        "        point_coords = torch.tensor([[float(x), float(y)]], dtype=torch.float32)\n",
        "        point_labels = torch.tensor([1], dtype=torch.int64)\n",
        "        return binary_mask, point_coords, point_labels\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        img, sem = self.base[idx]\n",
        "        img_t = self.img_transform(img)  # (3,H,W)\n",
        "        sem_t = torch.from_numpy(np.array(sem, dtype=np.int64))\n",
        "\n",
        "        sample = self._sample_prompt_from_mask(sem_t)\n",
        "        if sample is None:\n",
        "            # If something goes wrong, resample a different image.\n",
        "            return self[random.randrange(len(self))]\n",
        "\n",
        "        binary_mask, point_coords, point_labels = sample\n",
        "\n",
        "        return {\n",
        "            \"image\": img_t,\n",
        "            \"gt_mask\": binary_mask,        # (H,W)\n",
        "            \"point_coords\": point_coords,  # (1,2)\n",
        "            \"point_labels\": point_labels,  # (1,)\n",
        "        }\n",
        "\n",
        "\n",
        "def cityscapes_collate_fn(batch: list[Dict[str, Any]]) -> Dict[str, Tensor]:\n",
        "    \"\"\"\n",
        "    Simple collate for SAM-style training.\n",
        "    Assumes all images in batch have same size (true for Cityscapes).\n",
        "    \"\"\"\n",
        "    images = torch.stack([b[\"image\"] for b in batch], dim=0)             # (B,3,H,W)\n",
        "    gt_masks = torch.stack([b[\"gt_mask\"] for b in batch], dim=0)         # (B,H,W)\n",
        "    point_coords = torch.stack([b[\"point_coords\"] for b in batch], dim=0)  # (B,1,2)\n",
        "    point_labels = torch.stack([b[\"point_labels\"] for b in batch], dim=0)  # (B,1)\n",
        "\n",
        "    return {\n",
        "        \"image\": images,\n",
        "        \"gt_mask\": gt_masks,\n",
        "        \"point_coords\": point_coords,\n",
        "        \"point_labels\": point_labels,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "print(os.listdir(\".\")[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lcPqlrngAF6",
        "outputId": "6ae0f37a-df67-4b02-d345-fd53406d1432"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "['.config', 'efficientvit', 'drive', 'cityscapes_for_sam.py', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile losses_sam.py\n",
        "# losses_sam.py\n",
        "\"\"\"\n",
        "Loss functions for SAM-style training:\n",
        "- binary dice loss\n",
        "- binary focal loss\n",
        "- combined loss for multiple masks with best-of-N strategy.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def dice_loss(pred_probs: Tensor, target: Tensor, eps: float = 1e-6) -> Tensor:\n",
        "    \"\"\"\n",
        "    pred_probs, target: (B,1,H,W) in [0,1]\n",
        "    \"\"\"\n",
        "    intersection = (pred_probs * target).sum(dim=(2, 3))\n",
        "    union = pred_probs.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
        "    dice = (2.0 * intersection + eps) / (union + eps)\n",
        "    return 1.0 - dice.mean()\n",
        "\n",
        "\n",
        "def focal_loss(\n",
        "    pred_logits: Tensor,\n",
        "    target: Tensor,\n",
        "    alpha: float = 0.25,\n",
        "    gamma: float = 2.0,\n",
        "    eps: float = 1e-6,\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    pred_logits: (B,1,H,W) raw logits\n",
        "    target: (B,1,H,W) in {0,1}\n",
        "    \"\"\"\n",
        "    prob = torch.sigmoid(pred_logits)\n",
        "    pt = prob * target + (1 - prob) * (1 - target)\n",
        "    w = alpha * target + (1 - alpha) * (1 - target)\n",
        "    loss = -w * (1 - pt) ** gamma * torch.log(pt + eps)\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "def sam_mask_loss(multi_mask_logits: Tensor, gt_mask: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Compute SAM-style loss given multiple masks (e.g., 3) and a single GT mask.\n",
        "\n",
        "    Args:\n",
        "        multi_mask_logits: (B, M, H, W) predicted logits for M masks\n",
        "        gt_mask:           (B, H, W)     float {0,1}\n",
        "\n",
        "    Returns:\n",
        "        Scalar loss = mean over batch of:\n",
        "            min_m [ 20 * focal_loss + 1 * dice_loss ]\n",
        "    \"\"\"\n",
        "    B, M, H, W = multi_mask_logits.shape\n",
        "\n",
        "    # Expand gt to match masks.\n",
        "    gt = gt_mask.unsqueeze(1).expand(-1, M, -1, -1)  # (B,M,H,W)\n",
        "\n",
        "    # Compute mask-wise losses.\n",
        "    logits_flat = multi_mask_logits.view(B * M, 1, H, W)\n",
        "    gt_flat = gt.view(B * M, 1, H, W)\n",
        "\n",
        "    fl = focal_loss(logits_flat, gt_flat)  # scalar over all (B*M) if we do it this way\n",
        "\n",
        "    # If you want **strict** per-mask best-of-N, uncomment this more detailed version:\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #     prob_flat = torch.sigmoid(logits_flat)\n",
        "    # dl_per = dice_loss(prob_flat, gt_flat)  # currently scalar; reimplement to get per-sample if desired\n",
        "\n",
        "    # For a skeleton we keep it simple and just combine globally:\n",
        "    prob_flat = torch.sigmoid(logits_flat)\n",
        "    dl = dice_loss(prob_flat, gt_flat)\n",
        "\n",
        "    total = 20.0 * fl + 1.0 * dl\n",
        "    return total\n",
        "\n",
        "    # NOTE: If you want exact SAM behavior, modify this function to:\n",
        "    #   - compute focal+dice per sample & per mask (shape (B,M))\n",
        "    #   - take min over M for each sample\n",
        "    #   - average over B\n"
      ],
      "metadata": {
        "id": "YqImY7T90dO5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile sam_forward_train.py\n",
        "# sam_forward_train.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "# Adjust imports to match the EfficientViT-SAM repo\n",
        "from efficientvit.models.efficientvit.sam import EfficientViTSamPredictor\n",
        "\n",
        "\n",
        "def sam_forward_train(\n",
        "    predictor: EfficientViTSamPredictor,\n",
        "    batch: Dict[str, Tensor],\n",
        "    multimask_output: bool = True,\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    Training wrapper around EfficientViTSamPredictor.\n",
        "\n",
        "    Args:\n",
        "        predictor: EfficientViTSamPredictor instance wrapping EfficientViTSam.\n",
        "        batch: dict with keys:\n",
        "            - \"image\":        (B,3,H,W) float\n",
        "            - \"point_coords\": (B,1,2)  in pixel coords (same frame as image)\n",
        "            - \"point_labels\": (B,1)    int {0,1}\n",
        "        multimask_output: whether to ask the decoder for 3 masks (SAM style).\n",
        "\n",
        "    Returns:\n",
        "        multi_mask_logits: (B, M, H, W) logits in image resolution.\n",
        "    \"\"\"\n",
        "    device = predictor.device\n",
        "\n",
        "    images: Tensor = batch[\"image\"].to(device)              # (B,3,H,W)\n",
        "    point_coords: Tensor = batch[\"point_coords\"].to(device) # (B,1,2)\n",
        "    point_labels: Tensor = batch[\"point_labels\"].to(device) # (B,1)\n",
        "\n",
        "    B, C, H, W = images.shape\n",
        "\n",
        "    # ---- Prepare predictor state for this batch ----\n",
        "    # We bypass set_image/set_image_batch to keep gradients through image_encoder\n",
        "    # (in case you ever unfreeze it / add LoRA there).\n",
        "    predictor.reset_image()\n",
        "    predictor.original_size = (H, W)\n",
        "    predictor.input_size = (H, W)   # no resize: input frame == original frame\n",
        "\n",
        "    # Run image encoder with gradients enabled\n",
        "    predictor.features = predictor.model.image_encoder(images)\n",
        "    predictor.is_image_set = True\n",
        "\n",
        "    # ---- Run prompt encoder + mask decoder via predict_torch ----\n",
        "    # predict_torch expects coords already in the \"input_size\" frame; since\n",
        "    # we didn't resize, we can pass them directly.\n",
        "    masks, iou_predictions, low_res_masks = predictor.predict_torch(\n",
        "        point_coords=point_coords,       # (B,1,2)\n",
        "        point_labels=point_labels,       # (B,1)\n",
        "        boxes=None,\n",
        "        mask_input=None,\n",
        "        multimask_output=multimask_output,\n",
        "        return_logits=True,              # important: get logits, not thresholded masks\n",
        "        image_index=None,\n",
        "    )\n",
        "\n",
        "    # masks: (B, M, H, W) logits at original resolution\n",
        "    return masks\n"
      ],
      "metadata": {
        "id": "KQsHc5cewcn1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/assets/checkpoints/efficientvit_sam"
      ],
      "metadata": {
        "id": "u4wVR-eAoeri"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"efficientvit_sam_l0.pt\" -n /content/efficientvit/efficientvit/sam_model_zoo.py\n",
        "!grep -n \"http\" -n /content/efficientvit/efficientvit/sam_model_zoo.py | head -n 50\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnB4nwDZoiWT",
        "outputId": "006cc8e4-db51-4bc0-eaea-282fc649ba80"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18:    \"efficientvit-sam-l0\": (efficientvit_sam_l0, 1e-6, \"assets/checkpoints/efficientvit_sam/efficientvit_sam_l0.pt\"),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/assets/checkpoints/efficientvit_sam/efficientvit_sam_l0.pt \\\n",
        "https://huggingface.co/mit-han-lab/efficientvit-sam/resolve/main/efficientvit_sam_l0.pt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs0L2akNow5O",
        "outputId": "e9d727a3-c692-42ca-8ad9-76547cda766d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-13 21:34:38--  https://huggingface.co/mit-han-lab/efficientvit-sam/resolve/main/efficientvit_sam_l0.pt\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.34, 13.35.202.121, 13.35.202.97, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/6518760153c98221043c1a62/815ab8b6cf936422375be88ba5ad17d27ec8fc35f6bc80f8d064150b243d8991?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251213%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251213T213439Z&X-Amz-Expires=3600&X-Amz-Signature=96f067a5db466582268dcd054c074917bae08022d9b59d0a3e2202be3242a11f&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27efficientvit_sam_l0.pt%3B+filename%3D%22efficientvit_sam_l0.pt%22%3B&x-id=GetObject&Expires=1765665279&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTY2NTI3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTE4NzYwMTUzYzk4MjIxMDQzYzFhNjIvODE1YWI4YjZjZjkzNjQyMjM3NWJlODhiYTVhZDE3ZDI3ZWM4ZmMzNWY2YmM4MGY4ZDA2NDE1MGIyNDNkODk5MSoifV19&Signature=YtLjrJ6yaS424ZT2hZLbCzZ1NRAifia3zQp3FqQADEWysyDZe4%7EU1cpOiDLjS-kBSiz-k7hyXBdsrcC3ElCsvSDG0LexUKIgQt415vziT%7ENS6-a-ZIVYPkrTF6MjZmU%7EVHtco%7EPmMXXLgylOXN%7EBUQ9VAZeqyGcLcGJpZxsfpClX9yCG4-sEDG-nHxCs1lrVn8B5wad0I0F1Pob36SaI-HfQUiyb3GxVD9fI8cOnI3myGPmgoupwp38CG5wirFknMgmTtVsxCamRbEGQS1nczjRT5sCXs-wVANWdNL3xpGZVzCVJpFAgYjut4zOBTddaxcvG7m6-uE3eLZ2qBHfn4A__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-12-13 21:34:39--  https://cas-bridge.xethub.hf.co/xet-bridge-us/6518760153c98221043c1a62/815ab8b6cf936422375be88ba5ad17d27ec8fc35f6bc80f8d064150b243d8991?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251213%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251213T213439Z&X-Amz-Expires=3600&X-Amz-Signature=96f067a5db466582268dcd054c074917bae08022d9b59d0a3e2202be3242a11f&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27efficientvit_sam_l0.pt%3B+filename%3D%22efficientvit_sam_l0.pt%22%3B&x-id=GetObject&Expires=1765665279&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTY2NTI3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTE4NzYwMTUzYzk4MjIxMDQzYzFhNjIvODE1YWI4YjZjZjkzNjQyMjM3NWJlODhiYTVhZDE3ZDI3ZWM4ZmMzNWY2YmM4MGY4ZDA2NDE1MGIyNDNkODk5MSoifV19&Signature=YtLjrJ6yaS424ZT2hZLbCzZ1NRAifia3zQp3FqQADEWysyDZe4%7EU1cpOiDLjS-kBSiz-k7hyXBdsrcC3ElCsvSDG0LexUKIgQt415vziT%7ENS6-a-ZIVYPkrTF6MjZmU%7EVHtco%7EPmMXXLgylOXN%7EBUQ9VAZeqyGcLcGJpZxsfpClX9yCG4-sEDG-nHxCs1lrVn8B5wad0I0F1Pob36SaI-HfQUiyb3GxVD9fI8cOnI3myGPmgoupwp38CG5wirFknMgmTtVsxCamRbEGQS1nczjRT5sCXs-wVANWdNL3xpGZVzCVJpFAgYjut4zOBTddaxcvG7m6-uE3eLZ2qBHfn4A__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.67.175.25, 18.67.175.121, 18.67.175.108, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.67.175.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 139410184 (133M)\n",
            "Saving to: ‘/content/assets/checkpoints/efficientvit_sam/efficientvit_sam_l0.pt’\n",
            "\n",
            "/content/assets/che 100%[===================>] 132.95M  24.3MB/s    in 6.0s    \n",
            "\n",
            "2025-12-13 21:34:45 (22.1 MB/s) - ‘/content/assets/checkpoints/efficientvit_sam/efficientvit_sam_l0.pt’ saved [139410184/139410184]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile train_efficientvit_sam_cityscapes.py\n",
        "# train_efficientvit_sam_cityscapes.py\n",
        "\"\"\"\n",
        "Head-only fine-tuning of EfficientViT-SAM on Cityscapes.\n",
        "\n",
        "Reuses:\n",
        "    - EfficientViTSam model\n",
        "    - EfficientViTSamPredictor for preprocessing & inference\n",
        "and only adds:\n",
        "    - Cityscapes dataset wrapper\n",
        "    - training loop\n",
        "    - SAM loss\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "from typing import Dict, Any\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from cityscapes_for_sam import CityscapesForSAM, cityscapes_collate_fn\n",
        "from losses_sam import sam_mask_loss\n",
        "from sam_forward_train import sam_forward_train  # or local function\n",
        "\n",
        "# Adjust to your repo entry point\n",
        "from efficientvit.sam_model_zoo import create_efficientvit_sam_model\n",
        "from efficientvit.models.efficientvit.sam import EfficientViTSamPredictor\n",
        "\n",
        "\n",
        "def build_model_and_predictor(\n",
        "    device: str,\n",
        "    model_name: str = \"efficientvit-sam-l0\",\n",
        "    weights_path: str | None = None,\n",
        ") -> tuple[nn.Module, EfficientViTSamPredictor]:\n",
        "    \"\"\"\n",
        "    Build EfficientViT-SAM model + predictor and set requires_grad flags.\n",
        "    \"\"\"\n",
        "    # Example; adjust args to match repo\n",
        "    model = create_efficientvit_sam_model(name=model_name, pretrained=True)\n",
        "    model.to(device)\n",
        "\n",
        "    # Freeze image encoder\n",
        "    for p in model.image_encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Unfreeze mask decoder (head)\n",
        "    for p in model.mask_decoder.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    # Optionally also train prompt encoder\n",
        "    for p in model.prompt_encoder.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    predictor = EfficientViTSamPredictor(model)\n",
        "    return model, predictor\n",
        "\n",
        "\n",
        "def train(\n",
        "    cityscapes_root: str,\n",
        "    output_path: str = \"efficientvit_sam_head_finetuned_cityscapes.pt\",\n",
        "    model_name: str = \"efficientvit-sam-l0\",\n",
        "    batch_size: int = 2,\n",
        "    num_epochs: int = 10,\n",
        "    lr: float = 1e-4,\n",
        "    num_workers: int = 4,\n",
        ") -> None:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Datasets & loaders\n",
        "    train_ds = CityscapesForSAM(cityscapes_root, split=\"train\")\n",
        "    val_ds = CityscapesForSAM(cityscapes_root, split=\"val\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=cityscapes_collate_fn,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=cityscapes_collate_fn,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    # Model + predictor\n",
        "    model, predictor = build_model_and_predictor(\n",
        "        device=device,\n",
        "        model_name=model_name,\n",
        "        weights_path=None,  # or a checkpoint path if needed\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr=lr,\n",
        "        weight_decay=1e-2,\n",
        "    )\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # ----------------- Train -----------------\n",
        "        model.train()\n",
        "        train_loss_accum = 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [train]\"):\n",
        "            # Move GT to device\n",
        "            batch = {k: v.to(device) if isinstance(v, Tensor) else v for k, v in batch.items()}\n",
        "\n",
        "            # Forward through SAM predictor (you must implement sam_forward_train)\n",
        "            multi_mask_logits = sam_forward_train(predictor, batch)  # (B,M,H,W)\n",
        "\n",
        "            loss = sam_mask_loss(multi_mask_logits, batch[\"gt_mask\"])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_accum += loss.item()\n",
        "\n",
        "        train_loss = train_loss_accum / max(1, len(train_loader))\n",
        "        print(f\"[Epoch {epoch+1}] train loss: {train_loss:.4f}\")\n",
        "\n",
        "        # ----------------- Validation -----------------\n",
        "        model.eval()\n",
        "        val_loss_accum = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [val]\"):\n",
        "                batch = {k: v.to(device) if isinstance(v, Tensor) else v for k, v in batch.items()}\n",
        "                multi_mask_logits = sam_forward_train(predictor, batch)\n",
        "                loss = sam_mask_loss(multi_mask_logits, batch[\"gt_mask\"])\n",
        "                val_loss_accum += loss.item()\n",
        "\n",
        "        val_loss = val_loss_accum / max(1, len(val_loader))\n",
        "        print(f\"[Epoch {epoch+1}] val loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), output_path)\n",
        "            print(f\"  -> Saved new best model to {output_path}\")\n",
        "\n",
        "    print(f\"Training complete. Best val loss = {best_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # CHANGE THIS to your Cityscapes root directory.\n",
        "    cityscapes_root = \"/content/drive/MyDrive/datasets\"\n",
        "    train(cityscapes_root)\n"
      ],
      "metadata": {
        "id": "3FUTNoC10iJO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "6cb3b973-3181-4a4b-b063-3115855e09d7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [train]:   0%|          | 0/1488 [00:03<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/content/cityscapes_for_sam.py\", line 79, in __getitem__\n    sem_t = torch.as_tensor(sem, dtype=torch.long)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: 'PngImageFile' object cannot be interpreted as an integer\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2207939215.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# CHANGE THIS to your Cityscapes root directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mcityscapes_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/datasets\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcityscapes_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2207939215.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cityscapes_root, output_path, model_name, batch_size, num_epochs, lr, num_workers)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mtrain_loss_accum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs} [train]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0;31m# Move GT to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1504\u001b[0m                 \u001b[0mworker_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data, worker_idx)\u001b[0m\n\u001b[1;32m   1539\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# be constructed, don't try to instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/content/cityscapes_for_sam.py\", line 79, in __getitem__\n    sem_t = torch.as_tensor(sem, dtype=torch.long)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: 'PngImageFile' object cannot be interpreted as an integer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import efficientvit.sam_model_zoo as zoo\n",
        "print([name for name in dir(zoo) if \"sam\" in name.lower() or \"model\" in name.lower()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SftI_4GelIY6",
        "outputId": "c5df3ed9-656b-4725-a269-00d7702b9442"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['EfficientViTSam', 'REGISTERED_EFFICIENTVIT_SAM_MODEL', 'create_efficientvit_sam_model', 'efficientvit_sam_l0', 'efficientvit_sam_l1', 'efficientvit_sam_l2', 'efficientvit_sam_xl0', 'efficientvit_sam_xl1']\n"
          ]
        }
      ]
    }
  ]
}