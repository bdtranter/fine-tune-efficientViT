{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdtranter/fine-tune-efficientViT/blob/main/Tuned_EfficientViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQB8xaiS0aq2"
      },
      "outputs": [],
      "source": [
        "# cityscapes_for_sam.py\n",
        "\"\"\"\n",
        "Cityscapes wrapper that produces:\n",
        "    - image tensor\n",
        "    - one binary mask\n",
        "    - one positive point prompt\n",
        "for training a SAM-style model.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import random\n",
        "from typing import Dict, Any, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets import Cityscapes\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "class CityscapesForSAM(Dataset):\n",
        "    \"\"\"\n",
        "    Wraps torchvision Cityscapes and returns:\n",
        "        image: (3,H,W) float [0,1]\n",
        "        gt_mask: (H,W) float {0,1}\n",
        "        point_coords: (1,2) float [[x, y]] in pixel coords\n",
        "        point_labels: (1,) long [1]  (positive)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root: str, split: str = \"train\"):\n",
        "        super().__init__()\n",
        "        self.base = Cityscapes(\n",
        "            root=root,\n",
        "            split=split,\n",
        "            mode=\"fine\",\n",
        "            target_type=\"semantic\",\n",
        "        )\n",
        "        self.img_transform = T.ToTensor()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.base)\n",
        "\n",
        "    def _sample_prompt_from_mask(\n",
        "        self, sem_mask: Tensor\n",
        "    ) -> Optional[Tuple[Tensor, Tensor, Tensor]]:\n",
        "        \"\"\"\n",
        "        sem_mask: (H,W) long semantic labels.\n",
        "\n",
        "        Returns:\n",
        "            binary_mask: (H,W) float {0,1}\n",
        "            point_coords: (1,2) float [[x, y]]\n",
        "            point_labels: (1,) long [1]\n",
        "        or None if we fail to sample.\n",
        "        \"\"\"\n",
        "        # Remove background / ignore labels as needed.\n",
        "        classes = torch.unique(sem_mask)\n",
        "        classes = classes[(classes != 0) & (classes != 255)]\n",
        "        if len(classes) == 0:\n",
        "            return None\n",
        "\n",
        "        cls = classes[torch.randint(len(classes), (1,))]\n",
        "        region = (sem_mask == cls)\n",
        "        ys, xs = region.nonzero(as_tuple=True)\n",
        "        if len(xs) == 0:\n",
        "            return None\n",
        "\n",
        "        idx = torch.randint(len(xs), (1,))\n",
        "        y, x = ys[idx], xs[idx]\n",
        "\n",
        "        binary_mask = region.float()\n",
        "        point_coords = torch.tensor([[float(x), float(y)]], dtype=torch.float32)\n",
        "        point_labels = torch.tensor([1], dtype=torch.int64)\n",
        "        return binary_mask, point_coords, point_labels\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        img, sem = self.base[idx]\n",
        "        img_t = self.img_transform(img)  # (3,H,W)\n",
        "        sem_t = torch.as_tensor(sem, dtype=torch.long)\n",
        "\n",
        "        sample = self._sample_prompt_from_mask(sem_t)\n",
        "        if sample is None:\n",
        "            # If something goes wrong, resample a different image.\n",
        "            return self[random.randrange(len(self))]\n",
        "\n",
        "        binary_mask, point_coords, point_labels = sample\n",
        "\n",
        "        return {\n",
        "            \"image\": img_t,\n",
        "            \"gt_mask\": binary_mask,        # (H,W)\n",
        "            \"point_coords\": point_coords,  # (1,2)\n",
        "            \"point_labels\": point_labels,  # (1,)\n",
        "        }\n",
        "\n",
        "\n",
        "def cityscapes_collate_fn(batch: list[Dict[str, Any]]) -> Dict[str, Tensor]:\n",
        "    \"\"\"\n",
        "    Simple collate for SAM-style training.\n",
        "    Assumes all images in batch have same size (true for Cityscapes).\n",
        "    \"\"\"\n",
        "    images = torch.stack([b[\"image\"] for b in batch], dim=0)             # (B,3,H,W)\n",
        "    gt_masks = torch.stack([b[\"gt_mask\"] for b in batch], dim=0)         # (B,H,W)\n",
        "    point_coords = torch.stack([b[\"point_coords\"] for b in batch], dim=0)  # (B,1,2)\n",
        "    point_labels = torch.stack([b[\"point_labels\"] for b in batch], dim=0)  # (B,1)\n",
        "\n",
        "    return {\n",
        "        \"image\": images,\n",
        "        \"gt_mask\": gt_masks,\n",
        "        \"point_coords\": point_coords,\n",
        "        \"point_labels\": point_labels,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# losses_sam.py\n",
        "\"\"\"\n",
        "Loss functions for SAM-style training:\n",
        "- binary dice loss\n",
        "- binary focal loss\n",
        "- combined loss for multiple masks with best-of-N strategy.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def dice_loss(pred_probs: Tensor, target: Tensor, eps: float = 1e-6) -> Tensor:\n",
        "    \"\"\"\n",
        "    pred_probs, target: (B,1,H,W) in [0,1]\n",
        "    \"\"\"\n",
        "    intersection = (pred_probs * target).sum(dim=(2, 3))\n",
        "    union = pred_probs.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
        "    dice = (2.0 * intersection + eps) / (union + eps)\n",
        "    return 1.0 - dice.mean()\n",
        "\n",
        "\n",
        "def focal_loss(\n",
        "    pred_logits: Tensor,\n",
        "    target: Tensor,\n",
        "    alpha: float = 0.25,\n",
        "    gamma: float = 2.0,\n",
        "    eps: float = 1e-6,\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    pred_logits: (B,1,H,W) raw logits\n",
        "    target: (B,1,H,W) in {0,1}\n",
        "    \"\"\"\n",
        "    prob = torch.sigmoid(pred_logits)\n",
        "    pt = prob * target + (1 - prob) * (1 - target)\n",
        "    w = alpha * target + (1 - alpha) * (1 - target)\n",
        "    loss = -w * (1 - pt) ** gamma * torch.log(pt + eps)\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "def sam_mask_loss(multi_mask_logits: Tensor, gt_mask: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Compute SAM-style loss given multiple masks (e.g., 3) and a single GT mask.\n",
        "\n",
        "    Args:\n",
        "        multi_mask_logits: (B, M, H, W) predicted logits for M masks\n",
        "        gt_mask:           (B, H, W)     float {0,1}\n",
        "\n",
        "    Returns:\n",
        "        Scalar loss = mean over batch of:\n",
        "            min_m [ 20 * focal_loss + 1 * dice_loss ]\n",
        "    \"\"\"\n",
        "    B, M, H, W = multi_mask_logits.shape\n",
        "\n",
        "    # Expand gt to match masks.\n",
        "    gt = gt_mask.unsqueeze(1).expand(-1, M, -1, -1)  # (B,M,H,W)\n",
        "\n",
        "    # Compute mask-wise losses.\n",
        "    logits_flat = multi_mask_logits.view(B * M, 1, H, W)\n",
        "    gt_flat = gt.view(B * M, 1, H, W)\n",
        "\n",
        "    fl = focal_loss(logits_flat, gt_flat)  # scalar over all (B*M) if we do it this way\n",
        "\n",
        "    # If you want **strict** per-mask best-of-N, uncomment this more detailed version:\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #     prob_flat = torch.sigmoid(logits_flat)\n",
        "    # dl_per = dice_loss(prob_flat, gt_flat)  # currently scalar; reimplement to get per-sample if desired\n",
        "\n",
        "    # For a skeleton we keep it simple and just combine globally:\n",
        "    prob_flat = torch.sigmoid(logits_flat)\n",
        "    dl = dice_loss(prob_flat, gt_flat)\n",
        "\n",
        "    total = 20.0 * fl + 1.0 * dl\n",
        "    return total\n",
        "\n",
        "    # NOTE: If you want exact SAM behavior, modify this function to:\n",
        "    #   - compute focal+dice per sample & per mask (shape (B,M))\n",
        "    #   - take min over M for each sample\n",
        "    #   - average over B\n"
      ],
      "metadata": {
        "id": "YqImY7T90dO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sam_forward_train.py (optional) OR inside train script\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "# Adjust imports to match the EfficientViT-SAM repo\n",
        "from efficientvit.models.efficientvit.sam import EfficientViTSamPredictor\n",
        "\n",
        "\n",
        "def sam_forward_train(\n",
        "    predictor: \"EfficientViTSamPredictor\",\n",
        "    batch: Dict[str, Tensor],\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    Training wrapper around EfficientViTSamPredictor.\n",
        "\n",
        "    Args:\n",
        "        predictor: EfficientViTSamPredictor instance wrapping EfficientViTSam model.\n",
        "        batch: dict with keys:\n",
        "            - \"image\":        (B,3,H,W)\n",
        "            - \"point_coords\": (B,1,2) in original pixel coords\n",
        "            - \"point_labels\": (B,1)\n",
        "\n",
        "    Returns:\n",
        "        multi_mask_logits: (B, M, H, W) logits in original image resolution.\n",
        "\n",
        "    NOTE:\n",
        "        Here we assume you will call **existing internal methods** from the predictor\n",
        "        to avoid re-implementing preprocessing, embedding, and postprocessing.\n",
        "\n",
        "        Concretely, inside this function you should:\n",
        "          1. call a predictor method that:\n",
        "             - resizes & pads images\n",
        "             - transforms point_coords to resized space\n",
        "             - runs image_encoder + prompt_encoder + mask_decoder\n",
        "             - upscales masks back to original size\n",
        "          2. Convert probabilities -> logits via torch.logit.\n",
        "\n",
        "        Because the exact method names differ by repo, this function is left as a TODO.\n",
        "    \"\"\"\n",
        "    images = batch[\"image\"]\n",
        "    point_coords = batch[\"point_coords\"]\n",
        "    point_labels = batch[\"point_labels\"]\n",
        "\n",
        "    # TODO: Replace the following NotImplementedError with real calls.\n",
        "    # Example (pseudo-code, not guaranteed to match your repo):\n",
        "    #\n",
        "    # low_res_masks, iou_preds = predictor._predict_low_res_batch(\n",
        "    #     images, point_coords, point_labels\n",
        "    # )\n",
        "    # upscaled_probs = predictor.postprocess_masks_to_original_size(\n",
        "    #     low_res_masks, original_sizes=images.shape[-2:]\n",
        "    # )   # (B,M,H,W)\n",
        "    # logits = torch.logit(\n",
        "    #     upscaled_probs.clamp(1e-6, 1 - 1e-6),\n",
        "    #     eps=1e-6,\n",
        "    # )\n",
        "    # return logits\n",
        "\n",
        "    raise NotImplementedError(\"Hook this up to EfficientViTSamPredictor internals.\")\n"
      ],
      "metadata": {
        "id": "p6nBMNLW0gEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_efficientvit_sam_cityscapes.py\n",
        "\"\"\"\n",
        "Head-only fine-tuning of EfficientViT-SAM on Cityscapes.\n",
        "\n",
        "Reuses:\n",
        "    - EfficientViTSam model\n",
        "    - EfficientViTSamPredictor for preprocessing & inference\n",
        "and only adds:\n",
        "    - Cityscapes dataset wrapper\n",
        "    - training loop\n",
        "    - SAM loss\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "from typing import Dict, Any\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from cityscapes_for_sam import CityscapesForSAM, cityscapes_collate_fn\n",
        "from losses_sam import sam_mask_loss\n",
        "from sam_forward_train import sam_forward_train  # or local function\n",
        "\n",
        "# Adjust to your repo entry point\n",
        "from efficientvit.sam_model_zoo import create_sam_model\n",
        "from efficientvit.models.efficientvit.sam import EfficientViTSamPredictor\n",
        "\n",
        "\n",
        "def build_model_and_predictor(\n",
        "    device: str,\n",
        "    model_name: str = \"l0\",\n",
        "    weights_path: str | None = None,\n",
        ") -> tuple[nn.Module, EfficientViTSamPredictor]:\n",
        "    \"\"\"\n",
        "    Build EfficientViT-SAM model + predictor and set requires_grad flags.\n",
        "    \"\"\"\n",
        "    # Example; adjust args to match repo\n",
        "    model = create_sam_model(name=model_name, weight_url=weights_path)\n",
        "    model.to(device)\n",
        "\n",
        "    # Freeze image encoder\n",
        "    for p in model.image_encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Unfreeze mask decoder (head)\n",
        "    for p in model.mask_decoder.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    # Optionally also train prompt encoder\n",
        "    for p in model.prompt_encoder.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    predictor = EfficientViTSamPredictor(model)\n",
        "    return model, predictor\n",
        "\n",
        "\n",
        "def train(\n",
        "    cityscapes_root: str,\n",
        "    output_path: str = \"efficientvit_sam_head_finetuned_cityscapes.pt\",\n",
        "    model_name: str = \"l0\",\n",
        "    batch_size: int = 2,\n",
        "    num_epochs: int = 10,\n",
        "    lr: float = 1e-4,\n",
        "    num_workers: int = 4,\n",
        ") -> None:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Datasets & loaders\n",
        "    train_ds = CityscapesForSAM(cityscapes_root, split=\"train\")\n",
        "    val_ds = CityscapesForSAM(cityscapes_root, split=\"val\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=cityscapes_collate_fn,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=cityscapes_collate_fn,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    # Model + predictor\n",
        "    model, predictor = build_model_and_predictor(\n",
        "        device=device,\n",
        "        model_name=model_name,\n",
        "        weights_path=None,  # or a checkpoint path if needed\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr=lr,\n",
        "        weight_decay=1e-2,\n",
        "    )\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # ----------------- Train -----------------\n",
        "        model.train()\n",
        "        train_loss_accum = 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [train]\"):\n",
        "            # Move GT to device\n",
        "            batch = {k: v.to(device) if isinstance(v, Tensor) else v for k, v in batch.items()}\n",
        "\n",
        "            # Forward through SAM predictor (you must implement sam_forward_train)\n",
        "            multi_mask_logits = sam_forward_train(predictor, batch)  # (B,M,H,W)\n",
        "\n",
        "            loss = sam_mask_loss(multi_mask_logits, batch[\"gt_mask\"])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_accum += loss.item()\n",
        "\n",
        "        train_loss = train_loss_accum / max(1, len(train_loader))\n",
        "        print(f\"[Epoch {epoch+1}] train loss: {train_loss:.4f}\")\n",
        "\n",
        "        # ----------------- Validation -----------------\n",
        "        model.eval()\n",
        "        val_loss_accum = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [val]\"):\n",
        "                batch = {k: v.to(device) if isinstance(v, Tensor) else v for k, v in batch.items()}\n",
        "                multi_mask_logits = sam_forward_train(predictor, batch)\n",
        "                loss = sam_mask_loss(multi_mask_logits, batch[\"gt_mask\"])\n",
        "                val_loss_accum += loss.item()\n",
        "\n",
        "        val_loss = val_loss_accum / max(1, len(val_loader))\n",
        "        print(f\"[Epoch {epoch+1}] val loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), output_path)\n",
        "            print(f\"  -> Saved new best model to {output_path}\")\n",
        "\n",
        "    print(f\"Training complete. Best val loss = {best_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # CHANGE THIS to your Cityscapes root directory.\n",
        "    cityscapes_root = \"/path/to/cityscapes\"\n",
        "    train(cityscapes_root)\n"
      ],
      "metadata": {
        "id": "3FUTNoC10iJO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}