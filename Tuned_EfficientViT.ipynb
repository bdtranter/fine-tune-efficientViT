{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdtranter/fine-tune-efficientViT/blob/main/Tuned_EfficientViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data"
      ],
      "metadata": {
        "id": "34RIhGYT5QwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip once\n",
        "#\"-n\" tells unzip to skip existing files so extraction can safely resume without overwriting work already done.\n",
        "#This enables you to stop and resume the zip process\n",
        "!unzip -n \"/content/drive/MyDrive/datasets/gtFine_trainvaltest.zip\" -d \"/content/drive/MyDrive/datasets/\"\n",
        "!unzip -n \"/content/drive/MyDrive/datasets/leftImg8bit_trainvaltest.zip\" -d \"/content/drive/MyDrive/datasets/\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te5c6hwiDS0A",
        "outputId": "e7481617-f89a-4480-8a51-a33224cb2f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/datasets/leftImg8bit_trainvaltest.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "qiEY-2bu42Gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We1J9dIdCr13",
        "outputId": "da16f90c-ba26-4bcd-b36a-4d126a3e2504"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mit-han-lab/efficientvit.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwvJPix1XeFv",
        "outputId": "c70caf46-1a44-4f74-e0c8-06a974f98bda"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'efficientvit'...\n",
            "remote: Enumerating objects: 1524, done.\u001b[K\n",
            "remote: Counting objects: 100% (405/405), done.\u001b[K\n",
            "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
            "remote: Total 1524 (delta 296), reused 265 (delta 265), pack-reused 1119 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1524/1524), 217.43 MiB | 16.10 MiB/s, done.\n",
            "Resolving deltas: 100% (763/763), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/efficientvit\")"
      ],
      "metadata": {
        "id": "BnrJFVCobkAR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install timm einops opencv-python tqdm onnx onnxsim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1hy16Tybk5_",
        "outputId": "7c7a784e-6e27-49c9-ef15-c6c71fc1b373"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/21.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/21.0 MB\u001b[0m \u001b[31m229.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m16.4/21.0 MB\u001b[0m \u001b[31m254.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m236.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for onnxsim (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import einops\n",
        "import cv2\n",
        "import tqdm\n",
        "import onnx\n",
        "import onnxsim\n",
        "print(\"All imports successful ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUyWE5vsb62I",
        "outputId": "c9beef0f-5fe3-420c-a900-0978a68e3688"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports successful ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Install Checkpoint"
      ],
      "metadata": {
        "id": "nURdqEQ66cpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install git+https://github.com/facebookresearch/segment-anything.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyFnavMHfUWo",
        "outputId": "0e6782db-1095-40c9-d94e-47fa3e798746"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/assets/checkpoints/efficientvit_sam"
      ],
      "metadata": {
        "id": "u4wVR-eAoeri"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"efficientvit_sam_l0.pt\" -n /content/efficientvit/efficientvit/sam_model_zoo.py\n",
        "!grep -n \"http\" -n /content/efficientvit/efficientvit/sam_model_zoo.py | head -n 50\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnB4nwDZoiWT",
        "outputId": "c2b3cc52-7ac7-4ba0-cc75-944dae55635d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18:    \"efficientvit-sam-l0\": (efficientvit_sam_l0, 1e-6, \"assets/checkpoints/efficientvit_sam/efficientvit_sam_l0.pt\"),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/assets/checkpoints/efficientvit_sam/efficientvit_sam_l0.pt \\\n",
        "https://huggingface.co/mit-han-lab/efficientvit-sam/resolve/main/efficientvit_sam_l0.pt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs0L2akNow5O",
        "outputId": "7f9819ba-b2bc-41d5-e7df-96a6fdaf5cfa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-14 00:21:35--  https://huggingface.co/mit-han-lab/efficientvit-sam/resolve/main/efficientvit_sam_l0.pt\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.40, 13.35.202.34, 13.35.202.97, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/6518760153c98221043c1a62/815ab8b6cf936422375be88ba5ad17d27ec8fc35f6bc80f8d064150b243d8991?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251214%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251214T002135Z&X-Amz-Expires=3600&X-Amz-Signature=b48f2967a6188813b5fbcf214d3ed9973fa6dd9d178679be53cccc7f145a4ae5&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27efficientvit_sam_l0.pt%3B+filename%3D%22efficientvit_sam_l0.pt%22%3B&x-id=GetObject&Expires=1765675295&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTY3NTI5NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTE4NzYwMTUzYzk4MjIxMDQzYzFhNjIvODE1YWI4YjZjZjkzNjQyMjM3NWJlODhiYTVhZDE3ZDI3ZWM4ZmMzNWY2YmM4MGY4ZDA2NDE1MGIyNDNkODk5MSoifV19&Signature=ghnMDJHUzc1-%7E612%7Ejz1km2DRZmw-i1PMKtnWGNUPW8RCddrCWI%7ETK-LTF0Mn7XVP0MQWbt4zIlSmWNy68u8y7gmAQAEPhSlFD1GHdMCATDCZzZafyL9al99hsg2pQOQist2%7EoYyYWW7JyvhJ-9CO6mP-cRFU0kGmcYRl5LtQlIpSEfh-rX44uO-Ab7ayymG2Z9oSzf4I7o%7ERPhq44gop2TjJT22l40TNwkbhDpxcQCnvyzSsjuYpTCsiUqQ0tgW98n%7EvMoWrBfLmpAEVrjG8xatsHALSsTwTR%7EjZvn5u2LAuVmBoCPkGiojusVuzqhes7smU5eBlVzXDUHC2RGD4g__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-12-14 00:21:35--  https://cas-bridge.xethub.hf.co/xet-bridge-us/6518760153c98221043c1a62/815ab8b6cf936422375be88ba5ad17d27ec8fc35f6bc80f8d064150b243d8991?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251214%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251214T002135Z&X-Amz-Expires=3600&X-Amz-Signature=b48f2967a6188813b5fbcf214d3ed9973fa6dd9d178679be53cccc7f145a4ae5&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27efficientvit_sam_l0.pt%3B+filename%3D%22efficientvit_sam_l0.pt%22%3B&x-id=GetObject&Expires=1765675295&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTY3NTI5NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTE4NzYwMTUzYzk4MjIxMDQzYzFhNjIvODE1YWI4YjZjZjkzNjQyMjM3NWJlODhiYTVhZDE3ZDI3ZWM4ZmMzNWY2YmM4MGY4ZDA2NDE1MGIyNDNkODk5MSoifV19&Signature=ghnMDJHUzc1-%7E612%7Ejz1km2DRZmw-i1PMKtnWGNUPW8RCddrCWI%7ETK-LTF0Mn7XVP0MQWbt4zIlSmWNy68u8y7gmAQAEPhSlFD1GHdMCATDCZzZafyL9al99hsg2pQOQist2%7EoYyYWW7JyvhJ-9CO6mP-cRFU0kGmcYRl5LtQlIpSEfh-rX44uO-Ab7ayymG2Z9oSzf4I7o%7ERPhq44gop2TjJT22l40TNwkbhDpxcQCnvyzSsjuYpTCsiUqQ0tgW98n%7EvMoWrBfLmpAEVrjG8xatsHALSsTwTR%7EjZvn5u2LAuVmBoCPkGiojusVuzqhes7smU5eBlVzXDUHC2RGD4g__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.67.175.25, 18.67.175.108, 18.67.175.82, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.67.175.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 139410184 (133M)\n",
            "Saving to: ‘/content/assets/checkpoints/efficientvit_sam/efficientvit_sam_l0.pt’\n",
            "\n",
            "/content/assets/che 100%[===================>] 132.95M   203MB/s    in 0.7s    \n",
            "\n",
            "2025-12-14 00:21:36 (203 MB/s) - ‘/content/assets/checkpoints/efficientvit_sam/efficientvit_sam_l0.pt’ saved [139410184/139410184]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wHAkvrB4gnJU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fQB8xaiS0aq2"
      },
      "outputs": [],
      "source": [
        "#%%writefile cityscapes_for_sam.py\n",
        "# cityscapes_for_sam.py\n",
        "\"\"\"\n",
        "Cityscapes wrapper that produces:\n",
        "    - image tensor\n",
        "    - one binary mask\n",
        "    - one positive point prompt\n",
        "for training a SAM-style model.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import random\n",
        "from typing import Dict, Any, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets import Cityscapes\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CityscapesForSAM(Dataset):\n",
        "    \"\"\"\n",
        "    Wraps torchvision Cityscapes and returns:\n",
        "        image: (3,H,W) float [0,1]\n",
        "        gt_mask: (H,W) float {0,1}\n",
        "        point_coords: (1,2) float [[x, y]] in pixel coords\n",
        "        point_labels: (1,) long [1]  (positive)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root: str, split: str = \"train\"):\n",
        "        super().__init__()\n",
        "        self.base = Cityscapes(\n",
        "            root=root,\n",
        "            split=split,\n",
        "            mode=\"fine\",\n",
        "            target_type=\"semantic\",\n",
        "        )\n",
        "        self.img_transform = T.ToTensor()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.base)\n",
        "\n",
        "    def _sample_prompt_from_mask(\n",
        "        self, sem_mask: Tensor\n",
        "    ) -> Optional[Tuple[Tensor, Tensor, Tensor]]:\n",
        "        \"\"\"\n",
        "        sem_mask: (H,W) long semantic labels.\n",
        "\n",
        "        Returns:\n",
        "            binary_mask: (H,W) float {0,1}\n",
        "            point_coords: (1,2) float [[x, y]]\n",
        "            point_labels: (1,) long [1]\n",
        "        or None if we fail to sample.\n",
        "        \"\"\"\n",
        "        # Remove background / ignore labels as needed.\n",
        "        classes = torch.unique(sem_mask)\n",
        "        classes = classes[(classes != 0) & (classes != 255)]\n",
        "        if len(classes) == 0:\n",
        "            return None\n",
        "\n",
        "        cls = classes[torch.randint(len(classes), (1,))]\n",
        "        region = (sem_mask == cls)\n",
        "        ys, xs = region.nonzero(as_tuple=True)\n",
        "        if len(xs) == 0:\n",
        "            return None\n",
        "\n",
        "        idx = torch.randint(len(xs), (1,))\n",
        "        y, x = ys[idx], xs[idx]\n",
        "\n",
        "        binary_mask = region.float()\n",
        "        point_coords = torch.tensor([[float(x), float(y)]], dtype=torch.float32)\n",
        "        point_labels = torch.tensor([1], dtype=torch.int64)\n",
        "        return binary_mask, point_coords, point_labels\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        # DataLoader/Sampler sometimes passes numpy/torch scalar types; normalize them.\n",
        "        if not isinstance(idx, int):\n",
        "            idx = int(idx)\n",
        "        img, sem = self.base[idx]\n",
        "\n",
        "        # Force image -> RGB PIL (then ToTensor works reliably)\n",
        "        img = img.convert(\"RGB\")\n",
        "        img_t = T.ToTensor()(img)  # (3,H,W) float [0,1]\n",
        "\n",
        "        # Force semantic mask -> numpy int64 -> torch long\n",
        "        import numpy as np\n",
        "        sem_np = np.array(sem, dtype=np.int64)  # (H,W)\n",
        "        sem_t = torch.from_numpy(sem_np)        # long tensor\n",
        "\n",
        "        sample = self._sample_prompt_from_mask(sem_t)\n",
        "        if sample is None:\n",
        "            return self[random.randrange(len(self))]\n",
        "\n",
        "        binary_mask, point_coords, point_labels = sample\n",
        "        return {\n",
        "            \"image\": img_t,\n",
        "            \"gt_mask\": binary_mask,\n",
        "            \"point_coords\": point_coords,\n",
        "            \"point_labels\": point_labels,\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "def cityscapes_collate_fn(batch: list[Dict[str, Any]]) -> Dict[str, Tensor]:\n",
        "    \"\"\"\n",
        "    Simple collate for SAM-style training.\n",
        "    Assumes all images in batch have same size (true for Cityscapes).\n",
        "    \"\"\"\n",
        "    images = torch.stack([b[\"image\"] for b in batch], dim=0)             # (B,3,H,W)\n",
        "    gt_masks = torch.stack([b[\"gt_mask\"] for b in batch], dim=0)         # (B,H,W)\n",
        "    point_coords = torch.stack([b[\"point_coords\"] for b in batch], dim=0)  # (B,1,2)\n",
        "    point_labels = torch.stack([b[\"point_labels\"] for b in batch], dim=0)  # (B,1)\n",
        "\n",
        "    return {\n",
        "        \"image\": images,\n",
        "        \"gt_mask\": gt_masks,\n",
        "        \"point_coords\": point_coords,\n",
        "        \"point_labels\": point_labels,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GSWZl64W4gHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cityscapes_for_sam import CityscapesForSAM\n",
        "\n",
        "ds = CityscapesForSAM(\"/content/drive/MyDrive/datasets\", split=\"train\")\n",
        "\n",
        "print(\"len:\", len(ds))\n",
        "x = ds[0]\n",
        "print(\"ok keys:\", x.keys())\n",
        "print(\"image:\", x[\"image\"].shape, x[\"image\"].dtype)\n",
        "print(\"gt_mask:\", x[\"gt_mask\"].shape, x[\"gt_mask\"].dtype)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnJQbJQ6tRWi",
        "outputId": "d325e2c7-5d22-427f-c248-90f662b527d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len: 2975\n",
            "ok keys: dict_keys(['image', 'gt_mask', 'point_coords', 'point_labels'])\n",
            "image: torch.Size([3, 1024, 2048]) torch.float32\n",
            "gt_mask: torch.Size([1024, 2048]) torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TLyeF5uKxwAw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"idx variable currently is:\", \"idx\" in globals(), type(globals().get(\"idx\", None)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pysASuDVua_G",
        "outputId": "00789c41-dd04-4fc7-cf0f-9c8ad79cdaa9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx variable currently is: False <class 'NoneType'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "print(os.listdir(\".\")[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lcPqlrngAF6",
        "outputId": "6ae0f37a-df67-4b02-d345-fd53406d1432"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "['.config', 'efficientvit', 'drive', 'cityscapes_for_sam.py', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile losses_sam.py\n",
        "# losses_sam.py\n",
        "\"\"\"\n",
        "Loss functions for SAM-style training:\n",
        "- binary dice loss\n",
        "- binary focal loss\n",
        "- combined loss for multiple masks with best-of-N strategy.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def dice_loss(pred_probs: Tensor, target: Tensor, eps: float = 1e-6) -> Tensor:\n",
        "    \"\"\"\n",
        "    pred_probs, target: (B,1,H,W) in [0,1]\n",
        "    \"\"\"\n",
        "    intersection = (pred_probs * target).sum(dim=(2, 3))\n",
        "    union = pred_probs.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
        "    dice = (2.0 * intersection + eps) / (union + eps)\n",
        "    return 1.0 - dice.mean()\n",
        "\n",
        "\n",
        "def focal_loss(\n",
        "    pred_logits: Tensor,\n",
        "    target: Tensor,\n",
        "    alpha: float = 0.25,\n",
        "    gamma: float = 2.0,\n",
        "    eps: float = 1e-6,\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    pred_logits: (B,1,H,W) raw logits\n",
        "    target: (B,1,H,W) in {0,1}\n",
        "    \"\"\"\n",
        "    prob = torch.sigmoid(pred_logits)\n",
        "    pt = prob * target + (1 - prob) * (1 - target)\n",
        "    w = alpha * target + (1 - alpha) * (1 - target)\n",
        "    loss = -w * (1 - pt) ** gamma * torch.log(pt + eps)\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "def sam_mask_loss(multi_mask_logits: Tensor, gt_mask: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Compute SAM-style loss given multiple masks (e.g., 3) and a single GT mask.\n",
        "\n",
        "    Args:\n",
        "        multi_mask_logits: (B, M, H, W) predicted logits for M masks\n",
        "        gt_mask:           (B, H, W)     float {0,1}\n",
        "\n",
        "    Returns:\n",
        "        Scalar loss = mean over batch of:\n",
        "            min_m [ 20 * focal_loss + 1 * dice_loss ]\n",
        "    \"\"\"\n",
        "    B, M, H, W = multi_mask_logits.shape\n",
        "\n",
        "    # Expand gt to match masks.\n",
        "    gt = gt_mask.unsqueeze(1).expand(-1, M, -1, -1)  # (B,M,H,W)\n",
        "\n",
        "    # Compute mask-wise losses.\n",
        "    logits_flat = multi_mask_logits.view(B * M, 1, H, W)\n",
        "    gt_flat = gt.view(B * M, 1, H, W)\n",
        "\n",
        "    fl = focal_loss(logits_flat, gt_flat)  # scalar over all (B*M) if we do it this way\n",
        "\n",
        "    # If you want **strict** per-mask best-of-N, uncomment this more detailed version:\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #     prob_flat = torch.sigmoid(logits_flat)\n",
        "    # dl_per = dice_loss(prob_flat, gt_flat)  # currently scalar; reimplement to get per-sample if desired\n",
        "\n",
        "    # For a skeleton we keep it simple and just combine globally:\n",
        "    prob_flat = torch.sigmoid(logits_flat)\n",
        "    dl = dice_loss(prob_flat, gt_flat)\n",
        "\n",
        "    total = 20.0 * fl + 1.0 * dl\n",
        "    return total\n",
        "\n",
        "    # NOTE: If you want exact SAM behavior, modify this function to:\n",
        "    #   - compute focal+dice per sample & per mask (shape (B,M))\n",
        "    #   - take min over M for each sample\n",
        "    #   - average over B\n"
      ],
      "metadata": {
        "id": "YqImY7T90dO5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new version which bypasses predictors nograd\n",
        "#%%writefile sam_forward_train.py\n",
        "# sam_forward_train.py\n",
        "from __future__ import annotations\n",
        "from typing import Dict\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from efficientvit.models.efficientvit.sam import EfficientViTSamPredictor\n",
        "\n",
        "def sam_forward_train(\n",
        "    predictor: EfficientViTSamPredictor,\n",
        "    batch: Dict[str, Tensor],\n",
        "    multimask_output: bool = True,\n",
        ") -> Tensor:\n",
        "    device = predictor.device\n",
        "    model = predictor.model\n",
        "\n",
        "    images: Tensor = batch[\"image\"].to(device)              # (B,3,H,W)\n",
        "    point_coords: Tensor = batch[\"point_coords\"].to(device) # (B,1,2)\n",
        "    point_labels: Tensor = batch[\"point_labels\"].to(device) # (B,1)\n",
        "\n",
        "    B, _, H, W = images.shape\n",
        "\n",
        "    # --- image encoder (frozen, but we still need embeddings) ---\n",
        "    # If image_encoder is frozen, no grads needed here; keeps memory low.\n",
        "    with torch.no_grad():\n",
        "        image_embeddings = model.image_encoder(images)\n",
        "\n",
        "    # --- prompt encoder (trainable if you left it requires_grad=True) ---\n",
        "    sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
        "        points=(point_coords, point_labels),\n",
        "        boxes=None,\n",
        "        masks=None,\n",
        "    )\n",
        "\n",
        "    # --- mask decoder (trainable head) ---\n",
        "    low_res_masks, iou_predictions = model.mask_decoder(\n",
        "        image_embeddings=image_embeddings,\n",
        "        image_pe=model.prompt_encoder.get_dense_pe(),\n",
        "        sparse_prompt_embeddings=sparse_embeddings,\n",
        "        dense_prompt_embeddings=dense_embeddings,\n",
        "        multimask_output=multimask_output,\n",
        "    )\n",
        "\n",
        "    # low_res_masks is usually (B, M, 256, 256)\n",
        "    # Upsample to full image size to match gt_mask\n",
        "    masks = torch.nn.functional.interpolate(\n",
        "        low_res_masks,\n",
        "        size=(H, W),\n",
        "        mode=\"bilinear\",\n",
        "        align_corners=False,\n",
        "    )\n",
        "    return masks  # (B, M, H, W) logits\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "8pr5VdreOcbh",
        "outputId": "3800402f-00a8-4e7f-bf30-bc7983ce4955"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mInstalling onnxruntime by `/usr/bin/python3 -m pip install onnxruntime`, please wait for a moment..\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Installing onnxruntime by `/usr/bin/python3 -m pip install onnxruntime`, please wait for a moment..</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile sam_forward_train.py\n",
        "# sam_forward_train.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "# Adjust imports to match the EfficientViT-SAM repo\n",
        "from efficientvit.models.efficientvit.sam import EfficientViTSamPredictor\n",
        "\n",
        "\n",
        "def sam_forward_train(\n",
        "    predictor: EfficientViTSamPredictor,\n",
        "    batch: Dict[str, Tensor],\n",
        "    multimask_output: bool = True,\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    Training wrapper around EfficientViTSamPredictor.\n",
        "\n",
        "    Args:\n",
        "        predictor: EfficientViTSamPredictor instance wrapping EfficientViTSam.\n",
        "        batch: dict with keys:\n",
        "            - \"image\":        (B,3,H,W) float\n",
        "            - \"point_coords\": (B,1,2)  in pixel coords (same frame as image)\n",
        "            - \"point_labels\": (B,1)    int {0,1}\n",
        "        multimask_output: whether to ask the decoder for 3 masks (SAM style).\n",
        "\n",
        "    Returns:\n",
        "        multi_mask_logits: (B, M, H, W) logits in image resolution.\n",
        "    \"\"\"\n",
        "    device = predictor.device\n",
        "\n",
        "    images: Tensor = batch[\"image\"].to(device)              # (B,3,H,W)\n",
        "    point_coords: Tensor = batch[\"point_coords\"].to(device) # (B,1,2)\n",
        "    point_labels: Tensor = batch[\"point_labels\"].to(device) # (B,1)\n",
        "\n",
        "    B, C, H, W = images.shape\n",
        "\n",
        "    # ---- Prepare predictor state for this batch ----\n",
        "    # We bypass set_image/set_image_batch to keep gradients through image_encoder\n",
        "    # (in case you ever unfreeze it / add LoRA there).\n",
        "    predictor.reset_image()\n",
        "    predictor.original_size = (H, W)\n",
        "    predictor.input_size = (H, W)   # no resize: input frame == original frame\n",
        "\n",
        "    # Run image encoder with gradients enabled\n",
        "    predictor.features = predictor.model.image_encoder(images)\n",
        "    predictor.is_image_set = True\n",
        "\n",
        "    # ---- Run prompt encoder + mask decoder via predict_torch ----\n",
        "    # predict_torch expects coords already in the \"input_size\" frame; since\n",
        "    # we didn't resize, we can pass them directly.\n",
        "    masks, iou_predictions, low_res_masks = predictor.predict_torch(\n",
        "        point_coords=point_coords,       # (B,1,2)\n",
        "        point_labels=point_labels,       # (B,1)\n",
        "        boxes=None,\n",
        "        mask_input=None,\n",
        "        multimask_output=multimask_output,\n",
        "        return_logits=True,              # important: get logits, not thresholded masks\n",
        "        image_index=None,\n",
        "    )\n",
        "\n",
        "    # masks: (B, M, H, W) logits at original resolution\n",
        "    return masks\n"
      ],
      "metadata": {
        "id": "KQsHc5cewcn1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "8807735b-f0ef-47fc-a59a-4f30d911efbb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'efficientvit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1737720704.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Adjust imports to match the EfficientViT-SAM repo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mefficientvit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mefficientvit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msam\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEfficientViTSamPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'efficientvit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile train_efficientvit_sam_cityscapes.py\n",
        "# train_efficientvit_sam_cityscapes.py\n",
        "\"\"\"\n",
        "Head-only fine-tuning of EfficientViT-SAM on Cityscapes.\n",
        "\n",
        "Reuses:\n",
        "    - EfficientViTSam model\n",
        "    - EfficientViTSamPredictor for preprocessing & inference\n",
        "and only adds:\n",
        "    - Cityscapes dataset wrapper\n",
        "    - training loop\n",
        "    - SAM loss\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "from typing import Dict, Any\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from cityscapes_for_sam import CityscapesForSAM, cityscapes_collate_fn\n",
        "from losses_sam import sam_mask_loss\n",
        "from sam_forward_train import sam_forward_train  # or local function\n",
        "\n",
        "# Adjust to your repo entry point\n",
        "from efficientvit.sam_model_zoo import create_efficientvit_sam_model\n",
        "from efficientvit.models.efficientvit.sam import EfficientViTSamPredictor\n",
        "\n",
        "\n",
        "def build_model_and_predictor(\n",
        "    device: str,\n",
        "    model_name: str = \"efficientvit-sam-l0\",\n",
        "    weights_path: str | None = None,\n",
        ") -> tuple[nn.Module, EfficientViTSamPredictor]:\n",
        "    \"\"\"\n",
        "    Build EfficientViT-SAM model + predictor and set requires_grad flags.\n",
        "    \"\"\"\n",
        "    # Example; adjust args to match repo\n",
        "    model = create_efficientvit_sam_model(name=model_name, pretrained=True)\n",
        "    model.to(device)\n",
        "\n",
        "    # Freeze image encoder\n",
        "    for p in model.image_encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Unfreeze mask decoder (head)\n",
        "    for p in model.mask_decoder.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    # Optionally also train prompt encoder\n",
        "    for p in model.prompt_encoder.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    predictor = EfficientViTSamPredictor(model)\n",
        "    return model, predictor\n",
        "\n",
        "\n",
        "def train(\n",
        "    cityscapes_root: str,\n",
        "    output_path: str = \"efficientvit_sam_head_finetuned_cityscapes.pt\",\n",
        "    model_name: str = \"efficientvit-sam-l0\",\n",
        "    batch_size: int = 2,\n",
        "    num_epochs: int = 10,\n",
        "    lr: float = 1e-4,\n",
        "    num_workers: int = 4,\n",
        ") -> None:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Datasets & loaders\n",
        "    train_ds = CityscapesForSAM(cityscapes_root, split=\"train\")\n",
        "    val_ds = CityscapesForSAM(cityscapes_root, split=\"val\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=cityscapes_collate_fn,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=cityscapes_collate_fn,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    # Model + predictor\n",
        "    model, predictor = build_model_and_predictor(\n",
        "        device=device,\n",
        "        model_name=model_name,\n",
        "        weights_path=None,  # or a checkpoint path if needed\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr=lr,\n",
        "        weight_decay=1e-2,\n",
        "    )\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # ----------------- Train -----------------\n",
        "        model.train()\n",
        "        train_loss_accum = 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [train]\"):\n",
        "            # Move GT to device\n",
        "            batch = {k: v.to(device) if isinstance(v, Tensor) else v for k, v in batch.items()}\n",
        "\n",
        "            # Forward through SAM predictor (you must implement sam_forward_train)\n",
        "            multi_mask_logits = sam_forward_train(predictor, batch)  # (B,M,H,W)\n",
        "\n",
        "            loss = sam_mask_loss(multi_mask_logits, batch[\"gt_mask\"])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_accum += loss.item()\n",
        "\n",
        "        train_loss = train_loss_accum / max(1, len(train_loader))\n",
        "        print(f\"[Epoch {epoch+1}] train loss: {train_loss:.4f}\")\n",
        "\n",
        "        # ----------------- Validation -----------------\n",
        "        model.eval()\n",
        "        val_loss_accum = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [val]\"):\n",
        "                batch = {k: v.to(device) if isinstance(v, Tensor) else v for k, v in batch.items()}\n",
        "                multi_mask_logits = sam_forward_train(predictor, batch)\n",
        "                loss = sam_mask_loss(multi_mask_logits, batch[\"gt_mask\"])\n",
        "                val_loss_accum += loss.item()\n",
        "\n",
        "        val_loss = val_loss_accum / max(1, len(val_loader))\n",
        "        print(f\"[Epoch {epoch+1}] val loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), output_path)\n",
        "            print(f\"  -> Saved new best model to {output_path}\")\n",
        "\n",
        "    print(f\"Training complete. Best val loss = {best_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # CHANGE THIS to your Cityscapes root directory.\n",
        "    cityscapes_root = \"/content/drive/MyDrive/datasets\"\n",
        "    train(cityscapes_root, batch_size=1, num_workers=0)\n"
      ],
      "metadata": {
        "id": "3FUTNoC10iJO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "11bed58d-a1f5-448c-ce5e-96c4c4ceee48"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [train]:  13%|█▎        | 373/2975 [11:41<1:21:33,  1.88s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-122158464.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# CHANGE THIS to your Cityscapes root directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mcityscapes_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/datasets\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcityscapes_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-122158464.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cityscapes_root, output_path, model_name, batch_size, num_epochs, lr, num_workers)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mtrain_loss_accum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs} [train]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0;31m# Move GT to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/cityscapes_for_sam.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Force image -> RGB PIL (then ToTensor works reliably)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/cityscapes.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \"\"\"\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3522\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3524\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3526\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_mask_logits = sam_forward_train(predictor, batch)\n",
        "print(multi_mask_logits.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "kOEmpV2sNSEV",
        "outputId": "51580623-7d9d-4471-8525-eeb3fc2b63df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sam_forward_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1413404417.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmulti_mask_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msam_forward_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_mask_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sam_forward_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import efficientvit.sam_model_zoo as zoo\n",
        "print([name for name in dir(zoo) if \"sam\" in name.lower() or \"model\" in name.lower()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SftI_4GelIY6",
        "outputId": "c5df3ed9-656b-4725-a269-00d7702b9442"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['EfficientViTSam', 'REGISTERED_EFFICIENTVIT_SAM_MODEL', 'create_efficientvit_sam_model', 'efficientvit_sam_l0', 'efficientvit_sam_l1', 'efficientvit_sam_l2', 'efficientvit_sam_xl0', 'efficientvit_sam_xl1']\n"
          ]
        }
      ]
    }
  ]
}