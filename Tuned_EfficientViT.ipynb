{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdtranter/fine-tune-efficientViT/blob/main/Tuned_EfficientViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We1J9dIdCr13",
        "outputId": "dd56f289-1124-4e1e-deb4-cdfa4ad85735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mit-han-lab/efficientvit.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwvJPix1XeFv",
        "outputId": "a74c0804-3ed6-4bfd-c714-4220b9caf703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'efficientvit'...\n",
            "remote: Enumerating objects: 1524, done.\u001b[K\n",
            "remote: Counting objects: 100% (409/409), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 1524 (delta 299), reused 267 (delta 267), pack-reused 1115 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1524/1524), 217.43 MiB | 16.22 MiB/s, done.\n",
            "Resolving deltas: 100% (764/764), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/efficientvit\")"
      ],
      "metadata": {
        "id": "BnrJFVCobkAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install timm einops opencv-python tqdm onnx onnxsim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1hy16Tybk5_",
        "outputId": "61383926-0614-42df-cb21-2d38f538860f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for onnxsim (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import einops\n",
        "import cv2\n",
        "import tqdm\n",
        "print(\"All imports successful ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUyWE5vsb62I",
        "outputId": "cfdd51a1-0e89-4a1b-999c-f1326f20eba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports successful ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install git+https://github.com/facebookresearch/segment-anything.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyFnavMHfUWo",
        "outputId": "19f5b249-bb56-4890-ba6e-0fb31d73fb7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wHAkvrB4gnJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip once\n",
        "#\"-n\" tells unzip to skip existing files so extraction can safely resume without overwriting work already done.\n",
        "#This enables you to stop and resume the zip process\n",
        "!unzip -n \"/content/drive/MyDrive/datasets/gtFine_trainvaltest.zip\" -d \"/content/drive/MyDrive/datasets/\"\n",
        "!unzip -n \"/content/drive/MyDrive/datasets/leftImg8bit_trainvaltest.zip\" -d \"/content/drive/MyDrive/datasets/\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te5c6hwiDS0A",
        "outputId": "e7481617-f89a-4480-8a51-a33224cb2f2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/datasets/leftImg8bit_trainvaltest.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fQB8xaiS0aq2"
      },
      "outputs": [],
      "source": [
        "#%%writefile cityscapes_for_sam.py\n",
        "# cityscapes_for_sam.py\n",
        "\"\"\"\n",
        "Cityscapes wrapper that produces:\n",
        "    - image tensor\n",
        "    - one binary mask\n",
        "    - one positive point prompt\n",
        "for training a SAM-style model.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import random\n",
        "from typing import Dict, Any, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets import Cityscapes\n",
        "import torchvision.transforms as T\n",
        "\n",
        "\n",
        "class CityscapesForSAM(Dataset):\n",
        "    \"\"\"\n",
        "    Wraps torchvision Cityscapes and returns:\n",
        "        image: (3,H,W) float [0,1]\n",
        "        gt_mask: (H,W) float {0,1}\n",
        "        point_coords: (1,2) float [[x, y]] in pixel coords\n",
        "        point_labels: (1,) long [1]  (positive)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root: str, split: str = \"train\"):\n",
        "        super().__init__()\n",
        "        self.base = Cityscapes(\n",
        "            root=root,\n",
        "            split=split,\n",
        "            mode=\"fine\",\n",
        "            target_type=\"semantic\",\n",
        "        )\n",
        "        self.img_transform = T.ToTensor()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.base)\n",
        "\n",
        "    def _sample_prompt_from_mask(\n",
        "        self, sem_mask: Tensor\n",
        "    ) -> Optional[Tuple[Tensor, Tensor, Tensor]]:\n",
        "        \"\"\"\n",
        "        sem_mask: (H,W) long semantic labels.\n",
        "\n",
        "        Returns:\n",
        "            binary_mask: (H,W) float {0,1}\n",
        "            point_coords: (1,2) float [[x, y]]\n",
        "            point_labels: (1,) long [1]\n",
        "        or None if we fail to sample.\n",
        "        \"\"\"\n",
        "        # Remove background / ignore labels as needed.\n",
        "        classes = torch.unique(sem_mask)\n",
        "        classes = classes[(classes != 0) & (classes != 255)]\n",
        "        if len(classes) == 0:\n",
        "            return None\n",
        "\n",
        "        cls = classes[torch.randint(len(classes), (1,))]\n",
        "        region = (sem_mask == cls)\n",
        "        ys, xs = region.nonzero(as_tuple=True)\n",
        "        if len(xs) == 0:\n",
        "            return None\n",
        "\n",
        "        idx = torch.randint(len(xs), (1,))\n",
        "        y, x = ys[idx], xs[idx]\n",
        "\n",
        "        binary_mask = region.float()\n",
        "        point_coords = torch.tensor([[float(x), float(y)]], dtype=torch.float32)\n",
        "        point_labels = torch.tensor([1], dtype=torch.int64)\n",
        "        return binary_mask, point_coords, point_labels\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        img, sem = self.base[idx]\n",
        "        img_t = self.img_transform(img)  # (3,H,W)\n",
        "        sem_t = torch.as_tensor(sem, dtype=torch.long)\n",
        "\n",
        "        sample = self._sample_prompt_from_mask(sem_t)\n",
        "        if sample is None:\n",
        "            # If something goes wrong, resample a different image.\n",
        "            return self[random.randrange(len(self))]\n",
        "\n",
        "        binary_mask, point_coords, point_labels = sample\n",
        "\n",
        "        return {\n",
        "            \"image\": img_t,\n",
        "            \"gt_mask\": binary_mask,        # (H,W)\n",
        "            \"point_coords\": point_coords,  # (1,2)\n",
        "            \"point_labels\": point_labels,  # (1,)\n",
        "        }\n",
        "\n",
        "\n",
        "def cityscapes_collate_fn(batch: list[Dict[str, Any]]) -> Dict[str, Tensor]:\n",
        "    \"\"\"\n",
        "    Simple collate for SAM-style training.\n",
        "    Assumes all images in batch have same size (true for Cityscapes).\n",
        "    \"\"\"\n",
        "    images = torch.stack([b[\"image\"] for b in batch], dim=0)             # (B,3,H,W)\n",
        "    gt_masks = torch.stack([b[\"gt_mask\"] for b in batch], dim=0)         # (B,H,W)\n",
        "    point_coords = torch.stack([b[\"point_coords\"] for b in batch], dim=0)  # (B,1,2)\n",
        "    point_labels = torch.stack([b[\"point_labels\"] for b in batch], dim=0)  # (B,1)\n",
        "\n",
        "    return {\n",
        "        \"image\": images,\n",
        "        \"gt_mask\": gt_masks,\n",
        "        \"point_coords\": point_coords,\n",
        "        \"point_labels\": point_labels,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "print(os.listdir(\".\")[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lcPqlrngAF6",
        "outputId": "6ae0f37a-df67-4b02-d345-fd53406d1432"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "['.config', 'efficientvit', 'drive', 'cityscapes_for_sam.py', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile losses_sam.py\n",
        "# losses_sam.py\n",
        "\"\"\"\n",
        "Loss functions for SAM-style training:\n",
        "- binary dice loss\n",
        "- binary focal loss\n",
        "- combined loss for multiple masks with best-of-N strategy.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def dice_loss(pred_probs: Tensor, target: Tensor, eps: float = 1e-6) -> Tensor:\n",
        "    \"\"\"\n",
        "    pred_probs, target: (B,1,H,W) in [0,1]\n",
        "    \"\"\"\n",
        "    intersection = (pred_probs * target).sum(dim=(2, 3))\n",
        "    union = pred_probs.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
        "    dice = (2.0 * intersection + eps) / (union + eps)\n",
        "    return 1.0 - dice.mean()\n",
        "\n",
        "\n",
        "def focal_loss(\n",
        "    pred_logits: Tensor,\n",
        "    target: Tensor,\n",
        "    alpha: float = 0.25,\n",
        "    gamma: float = 2.0,\n",
        "    eps: float = 1e-6,\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    pred_logits: (B,1,H,W) raw logits\n",
        "    target: (B,1,H,W) in {0,1}\n",
        "    \"\"\"\n",
        "    prob = torch.sigmoid(pred_logits)\n",
        "    pt = prob * target + (1 - prob) * (1 - target)\n",
        "    w = alpha * target + (1 - alpha) * (1 - target)\n",
        "    loss = -w * (1 - pt) ** gamma * torch.log(pt + eps)\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "def sam_mask_loss(multi_mask_logits: Tensor, gt_mask: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Compute SAM-style loss given multiple masks (e.g., 3) and a single GT mask.\n",
        "\n",
        "    Args:\n",
        "        multi_mask_logits: (B, M, H, W) predicted logits for M masks\n",
        "        gt_mask:           (B, H, W)     float {0,1}\n",
        "\n",
        "    Returns:\n",
        "        Scalar loss = mean over batch of:\n",
        "            min_m [ 20 * focal_loss + 1 * dice_loss ]\n",
        "    \"\"\"\n",
        "    B, M, H, W = multi_mask_logits.shape\n",
        "\n",
        "    # Expand gt to match masks.\n",
        "    gt = gt_mask.unsqueeze(1).expand(-1, M, -1, -1)  # (B,M,H,W)\n",
        "\n",
        "    # Compute mask-wise losses.\n",
        "    logits_flat = multi_mask_logits.view(B * M, 1, H, W)\n",
        "    gt_flat = gt.view(B * M, 1, H, W)\n",
        "\n",
        "    fl = focal_loss(logits_flat, gt_flat)  # scalar over all (B*M) if we do it this way\n",
        "\n",
        "    # If you want **strict** per-mask best-of-N, uncomment this more detailed version:\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #     prob_flat = torch.sigmoid(logits_flat)\n",
        "    # dl_per = dice_loss(prob_flat, gt_flat)  # currently scalar; reimplement to get per-sample if desired\n",
        "\n",
        "    # For a skeleton we keep it simple and just combine globally:\n",
        "    prob_flat = torch.sigmoid(logits_flat)\n",
        "    dl = dice_loss(prob_flat, gt_flat)\n",
        "\n",
        "    total = 20.0 * fl + 1.0 * dl\n",
        "    return total\n",
        "\n",
        "    # NOTE: If you want exact SAM behavior, modify this function to:\n",
        "    #   - compute focal+dice per sample & per mask (shape (B,M))\n",
        "    #   - take min over M for each sample\n",
        "    #   - average over B\n"
      ],
      "metadata": {
        "id": "YqImY7T90dO5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile sam_forward_train.py\n",
        "# sam_forward_train.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "# Adjust imports to match the EfficientViT-SAM repo\n",
        "from efficientvit.models.efficientvit.sam import EfficientViTSamPredictor\n",
        "\n",
        "\n",
        "def sam_forward_train(\n",
        "    predictor: EfficientViTSamPredictor,\n",
        "    batch: Dict[str, Tensor],\n",
        "    multimask_output: bool = True,\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    Training wrapper around EfficientViTSamPredictor.\n",
        "\n",
        "    Args:\n",
        "        predictor: EfficientViTSamPredictor instance wrapping EfficientViTSam.\n",
        "        batch: dict with keys:\n",
        "            - \"image\":        (B,3,H,W) float\n",
        "            - \"point_coords\": (B,1,2)  in pixel coords (same frame as image)\n",
        "            - \"point_labels\": (B,1)    int {0,1}\n",
        "        multimask_output: whether to ask the decoder for 3 masks (SAM style).\n",
        "\n",
        "    Returns:\n",
        "        multi_mask_logits: (B, M, H, W) logits in image resolution.\n",
        "    \"\"\"\n",
        "    device = predictor.device\n",
        "\n",
        "    images: Tensor = batch[\"image\"].to(device)              # (B,3,H,W)\n",
        "    point_coords: Tensor = batch[\"point_coords\"].to(device) # (B,1,2)\n",
        "    point_labels: Tensor = batch[\"point_labels\"].to(device) # (B,1)\n",
        "\n",
        "    B, C, H, W = images.shape\n",
        "\n",
        "    # ---- Prepare predictor state for this batch ----\n",
        "    # We bypass set_image/set_image_batch to keep gradients through image_encoder\n",
        "    # (in case you ever unfreeze it / add LoRA there).\n",
        "    predictor.reset_image()\n",
        "    predictor.original_size = (H, W)\n",
        "    predictor.input_size = (H, W)   # no resize: input frame == original frame\n",
        "\n",
        "    # Run image encoder with gradients enabled\n",
        "    predictor.features = predictor.model.image_encoder(images)\n",
        "    predictor.is_image_set = True\n",
        "\n",
        "    # ---- Run prompt encoder + mask decoder via predict_torch ----\n",
        "    # predict_torch expects coords already in the \"input_size\" frame; since\n",
        "    # we didn't resize, we can pass them directly.\n",
        "    masks, iou_predictions, low_res_masks = predictor.predict_torch(\n",
        "        point_coords=point_coords,       # (B,1,2)\n",
        "        point_labels=point_labels,       # (B,1)\n",
        "        boxes=None,\n",
        "        mask_input=None,\n",
        "        multimask_output=multimask_output,\n",
        "        return_logits=True,              # important: get logits, not thresholded masks\n",
        "        image_index=None,\n",
        "    )\n",
        "\n",
        "    # masks: (B, M, H, W) logits at original resolution\n",
        "    return masks\n"
      ],
      "metadata": {
        "id": "KQsHc5cewcn1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile train_efficientvit_sam_cityscapes.py\n",
        "# train_efficientvit_sam_cityscapes.py\n",
        "\"\"\"\n",
        "Head-only fine-tuning of EfficientViT-SAM on Cityscapes.\n",
        "\n",
        "Reuses:\n",
        "    - EfficientViTSam model\n",
        "    - EfficientViTSamPredictor for preprocessing & inference\n",
        "and only adds:\n",
        "    - Cityscapes dataset wrapper\n",
        "    - training loop\n",
        "    - SAM loss\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "from typing import Dict, Any\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from cityscapes_for_sam import CityscapesForSAM, cityscapes_collate_fn\n",
        "from losses_sam import sam_mask_loss\n",
        "from sam_forward_train import sam_forward_train  # or local function\n",
        "\n",
        "# Adjust to your repo entry point\n",
        "from efficientvit.sam_model_zoo import create_efficientvit_sam_model\n",
        "from efficientvit.models.efficientvit.sam import EfficientViTSamPredictor\n",
        "\n",
        "\n",
        "def build_model_and_predictor(\n",
        "    device: str,\n",
        "    model_name: str = \"efficientvit-sam-l0\",\n",
        "    weights_path: str | None = None,\n",
        ") -> tuple[nn.Module, EfficientViTSamPredictor]:\n",
        "    \"\"\"\n",
        "    Build EfficientViT-SAM model + predictor and set requires_grad flags.\n",
        "    \"\"\"\n",
        "    # Example; adjust args to match repo\n",
        "    model = create_efficientvit_sam_model(name=model_name, weight_url=weights_path)\n",
        "    model.to(device)\n",
        "\n",
        "    # Freeze image encoder\n",
        "    for p in model.image_encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Unfreeze mask decoder (head)\n",
        "    for p in model.mask_decoder.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    # Optionally also train prompt encoder\n",
        "    for p in model.prompt_encoder.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    predictor = EfficientViTSamPredictor(model)\n",
        "    return model, predictor\n",
        "\n",
        "\n",
        "def train(\n",
        "    cityscapes_root: str,\n",
        "    output_path: str = \"efficientvit_sam_head_finetuned_cityscapes.pt\",\n",
        "    model_name: str = \"efficientvit-sam-l0\",\n",
        "    batch_size: int = 2,\n",
        "    num_epochs: int = 10,\n",
        "    lr: float = 1e-4,\n",
        "    num_workers: int = 4,\n",
        ") -> None:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Datasets & loaders\n",
        "    train_ds = CityscapesForSAM(cityscapes_root, split=\"train\")\n",
        "    val_ds = CityscapesForSAM(cityscapes_root, split=\"val\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=cityscapes_collate_fn,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=cityscapes_collate_fn,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    # Model + predictor\n",
        "    model, predictor = build_model_and_predictor(\n",
        "        device=device,\n",
        "        model_name=model_name,\n",
        "        weights_path=None,  # or a checkpoint path if needed\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr=lr,\n",
        "        weight_decay=1e-2,\n",
        "    )\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # ----------------- Train -----------------\n",
        "        model.train()\n",
        "        train_loss_accum = 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [train]\"):\n",
        "            # Move GT to device\n",
        "            batch = {k: v.to(device) if isinstance(v, Tensor) else v for k, v in batch.items()}\n",
        "\n",
        "            # Forward through SAM predictor (you must implement sam_forward_train)\n",
        "            multi_mask_logits = sam_forward_train(predictor, batch)  # (B,M,H,W)\n",
        "\n",
        "            loss = sam_mask_loss(multi_mask_logits, batch[\"gt_mask\"])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_accum += loss.item()\n",
        "\n",
        "        train_loss = train_loss_accum / max(1, len(train_loader))\n",
        "        print(f\"[Epoch {epoch+1}] train loss: {train_loss:.4f}\")\n",
        "\n",
        "        # ----------------- Validation -----------------\n",
        "        model.eval()\n",
        "        val_loss_accum = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [val]\"):\n",
        "                batch = {k: v.to(device) if isinstance(v, Tensor) else v for k, v in batch.items()}\n",
        "                multi_mask_logits = sam_forward_train(predictor, batch)\n",
        "                loss = sam_mask_loss(multi_mask_logits, batch[\"gt_mask\"])\n",
        "                val_loss_accum += loss.item()\n",
        "\n",
        "        val_loss = val_loss_accum / max(1, len(val_loader))\n",
        "        print(f\"[Epoch {epoch+1}] val loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), output_path)\n",
        "            print(f\"  -> Saved new best model to {output_path}\")\n",
        "\n",
        "    print(f\"Training complete. Best val loss = {best_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # CHANGE THIS to your Cityscapes root directory.\n",
        "    cityscapes_root = \"/content/drive/MyDrive/datasets\"\n",
        "    train(cityscapes_root)\n"
      ],
      "metadata": {
        "id": "3FUTNoC10iJO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "f6789c82-76ea-4c4d-96d3-a2754468f261"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/assets/checkpoints/efficientvit_sam/efficientvit_sam_l0.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1103463380.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# CHANGE THIS to your Cityscapes root directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mcityscapes_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/datasets\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcityscapes_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1103463380.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cityscapes_root, output_path, model_name, batch_size, num_epochs, lr, num_workers)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# Model + predictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     model, predictor = build_model_and_predictor(\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1103463380.py\u001b[0m in \u001b[0;36mbuild_model_and_predictor\u001b[0;34m(device, model_name, weights_path)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \"\"\"\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Example; adjust args to match repo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_efficientvit_sam_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/efficientvit/efficientvit/sam_model_zoo.py\u001b[0m in \u001b[0;36mcreate_efficientvit_sam_model\u001b[0;34m(name, pretrained, weight_url, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cannot find the pretrained weight of {name}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_state_dict_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/efficientvit/efficientvit/models/utils/network.py\u001b[0m in \u001b[0;36mload_state_dict_from_file\u001b[0;34m(file, only_state_dict)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_state_dict_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_state_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0monly_state_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"state_dict\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/assets/checkpoints/efficientvit_sam/efficientvit_sam_l0.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import efficientvit.sam_model_zoo as zoo\n",
        "print([name for name in dir(zoo) if \"sam\" in name.lower() or \"model\" in name.lower()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SftI_4GelIY6",
        "outputId": "c5df3ed9-656b-4725-a269-00d7702b9442"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['EfficientViTSam', 'REGISTERED_EFFICIENTVIT_SAM_MODEL', 'create_efficientvit_sam_model', 'efficientvit_sam_l0', 'efficientvit_sam_l1', 'efficientvit_sam_l2', 'efficientvit_sam_xl0', 'efficientvit_sam_xl1']\n"
          ]
        }
      ]
    }
  ]
}