{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bdtranter/fine-tune-efficientViT/blob/main/Tuned_EfficientViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mount Drive"
      ],
      "metadata": {
        "id": "gobEoYEH9AND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We1J9dIdCr13",
        "outputId": "6c328683-6285-4da3-d08e-88ee9f16e7da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extract Cityscapes"
      ],
      "metadata": {
        "id": "34RIhGYT5QwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#unzip once\n",
        "#\"-n\" tells unzip to skip existing files so extraction can safely resume without overwriting work already done.\n",
        "#This enables you to stop and resume the zip process\n",
        "!unzip -n \"/content/drive/MyDrive/datasets/gtFine_trainvaltest.zip\" -d \"/content/drive/MyDrive/datasets/\"\n",
        "!unzip -n \"/content/drive/MyDrive/datasets/leftImg8bit_trainvaltest.zip\" -d \"/content/drive/MyDrive/datasets/\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te5c6hwiDS0A",
        "outputId": "e7481617-f89a-4480-8a51-a33224cb2f2a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/datasets/leftImg8bit_trainvaltest.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "qiEY-2bu42Gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mit-han-lab/efficientvit.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwvJPix1XeFv",
        "outputId": "6ff75800-8d14-44a8-ee94-1f9e59aba2de"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'efficientvit'...\n",
            "remote: Enumerating objects: 1524, done.\u001b[K\n",
            "remote: Counting objects: 100% (409/409), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 1524 (delta 299), reused 267 (delta 267), pack-reused 1115 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1524/1524), 217.43 MiB | 40.70 MiB/s, done.\n",
            "Resolving deltas: 100% (764/764), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/efficientvit\")"
      ],
      "metadata": {
        "id": "BnrJFVCobkAR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install timm einops opencv-python tqdm onnx onnxsim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1hy16Tybk5_",
        "outputId": "a4df68fa-bc82-4521-fc25-696e95092bf4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for onnxsim (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import einops\n",
        "import cv2\n",
        "import tqdm\n",
        "import onnx\n",
        "import onnxsim\n",
        "print(\"All imports successful ✅\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "id": "HUyWE5vsb62I",
        "outputId": "ee040c95-ff76-469c-83ce-4fdf806ca49a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mInstalling onnxruntime by `/usr/bin/python3 -m pip install onnxruntime`, please wait for a moment..\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Installing onnxruntime by `/usr/bin/python3 -m pip install onnxruntime`, please wait for a moment..</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports successful ✅\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Install Checkpoint"
      ],
      "metadata": {
        "id": "nURdqEQ66cpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install git+https://github.com/facebookresearch/segment-anything.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyFnavMHfUWo",
        "outputId": "8b7dfde3-0f45-403b-82ea-63645e6eb7af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for segment_anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/assets/checkpoints/efficientvit_sam"
      ],
      "metadata": {
        "id": "u4wVR-eAoeri"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"efficientvit_sam_l0.pt\" -n /content/efficientvit/efficientvit/sam_model_zoo.py\n",
        "!grep -n \"http\" -n /content/efficientvit/efficientvit/sam_model_zoo.py | head -n 50\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnB4nwDZoiWT",
        "outputId": "babfd669-c887-4c52-df26-7898cbf07f40"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18:    \"efficientvit-sam-l0\": (efficientvit_sam_l0, 1e-6, \"assets/checkpoints/efficientvit_sam/efficientvit_sam_l0.pt\"),\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/assets/checkpoints/efficientvit_sam/efficientvit_sam_l0.pt \\\n",
        "https://huggingface.co/mit-han-lab/efficientvit-sam/resolve/main/efficientvit_sam_l0.pt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs0L2akNow5O",
        "outputId": "3d1db9df-f35f-4138-a25e-da2ddcde9613"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-14 17:49:54--  https://huggingface.co/mit-han-lab/efficientvit-sam/resolve/main/efficientvit_sam_l0.pt\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.118, 18.164.174.17, 18.164.174.55, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/6518760153c98221043c1a62/815ab8b6cf936422375be88ba5ad17d27ec8fc35f6bc80f8d064150b243d8991?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251214%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251214T174954Z&X-Amz-Expires=3600&X-Amz-Signature=3082f2e756f3adb566cc38058bb9dbbfca558fd4f8e9ae981606baa2b9a924f1&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27efficientvit_sam_l0.pt%3B+filename%3D%22efficientvit_sam_l0.pt%22%3B&x-id=GetObject&Expires=1765738194&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTczODE5NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTE4NzYwMTUzYzk4MjIxMDQzYzFhNjIvODE1YWI4YjZjZjkzNjQyMjM3NWJlODhiYTVhZDE3ZDI3ZWM4ZmMzNWY2YmM4MGY4ZDA2NDE1MGIyNDNkODk5MSoifV19&Signature=KFBFKt8iRKr-rgEPnZQZnrHgH%7EtIp%7ErnWdj6v3siOYXbNJcC-KYhkR2d2fla62-wFEU-pQ6UN0QtZD25pGHFLOAxrZWq20vNpNQCJKH8RJXVCVo2k1Lf8FRuChAzu5nptwjF-dQUuMo4JEnQG0ha1toVWJ50xL70umLC58ycTnMACan0BC-JvnP18qGWsEJKHBbbf%7EacsMlBNyjbbKCIoOwNDmXWgK3jvyQCn5vwwLENkQKtnBFi6Au-sLWek-CSpG0A7Va6j22iDZ9i1waMMWF2uu0s%7Ey7FEcPNYqUaQiowUNAKLXoxyCDkJrQBE-aGliIsvY4T-HfCUMB64Qm2jQ__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-12-14 17:49:54--  https://cas-bridge.xethub.hf.co/xet-bridge-us/6518760153c98221043c1a62/815ab8b6cf936422375be88ba5ad17d27ec8fc35f6bc80f8d064150b243d8991?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251214%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251214T174954Z&X-Amz-Expires=3600&X-Amz-Signature=3082f2e756f3adb566cc38058bb9dbbfca558fd4f8e9ae981606baa2b9a924f1&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27efficientvit_sam_l0.pt%3B+filename%3D%22efficientvit_sam_l0.pt%22%3B&x-id=GetObject&Expires=1765738194&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NTczODE5NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTE4NzYwMTUzYzk4MjIxMDQzYzFhNjIvODE1YWI4YjZjZjkzNjQyMjM3NWJlODhiYTVhZDE3ZDI3ZWM4ZmMzNWY2YmM4MGY4ZDA2NDE1MGIyNDNkODk5MSoifV19&Signature=KFBFKt8iRKr-rgEPnZQZnrHgH%7EtIp%7ErnWdj6v3siOYXbNJcC-KYhkR2d2fla62-wFEU-pQ6UN0QtZD25pGHFLOAxrZWq20vNpNQCJKH8RJXVCVo2k1Lf8FRuChAzu5nptwjF-dQUuMo4JEnQG0ha1toVWJ50xL70umLC58ycTnMACan0BC-JvnP18qGWsEJKHBbbf%7EacsMlBNyjbbKCIoOwNDmXWgK3jvyQCn5vwwLENkQKtnBFi6Au-sLWek-CSpG0A7Va6j22iDZ9i1waMMWF2uu0s%7Ey7FEcPNYqUaQiowUNAKLXoxyCDkJrQBE-aGliIsvY4T-HfCUMB64Qm2jQ__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 3.168.132.96, 3.168.132.31, 3.168.132.62, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|3.168.132.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 139410184 (133M)\n",
            "Saving to: ‘/content/assets/checkpoints/efficientvit_sam/efficientvit_sam_l0.pt’\n",
            "\n",
            "/content/assets/che 100%[===================>] 132.95M  71.8MB/s    in 1.9s    \n",
            "\n",
            "2025-12-14 17:49:56 (71.8 MB/s) - ‘/content/assets/checkpoints/efficientvit_sam/efficientvit_sam_l0.pt’ saved [139410184/139410184]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main Functions\n"
      ],
      "metadata": {
        "id": "wHAkvrB4gnJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cityscapes wrapper"
      ],
      "metadata": {
        "id": "qgiST41P8F82"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fQB8xaiS0aq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "012843f8-1993-489d-c380-08fd5e2fea2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cityscapes_for_sam.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile cityscapes_for_sam.py\n",
        "# cityscapes_for_sam.py\n",
        "\"\"\"\n",
        "Cityscapes wrapper that produces:\n",
        "    - image tensor\n",
        "    - one binary mask\n",
        "    - one positive point prompt\n",
        "for training a SAM-style model.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import random\n",
        "from typing import Dict, Any, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets import Cityscapes\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CityscapesForSAM(Dataset):\n",
        "    \"\"\"\n",
        "    Wraps torchvision Cityscapes and returns:\n",
        "        image: (3,H,W) float [0,1]\n",
        "        gt_mask: (H,W) float {0,1}\n",
        "        point_coords: (1,2) float [[x, y]] in pixel coords\n",
        "        point_labels: (1,) long [1]  (positive)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root: str, split: str = \"train\"):\n",
        "        super().__init__()\n",
        "        self.base = Cityscapes(\n",
        "            root=root,\n",
        "            split=split,\n",
        "            mode=\"fine\",\n",
        "            target_type=\"semantic\",\n",
        "        )\n",
        "        self.img_transform = T.ToTensor()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.base)\n",
        "\n",
        "    def _sample_prompt_from_mask(\n",
        "        self, sem_mask: Tensor\n",
        "    ) -> Optional[Tuple[Tensor, Tensor, Tensor]]:\n",
        "        \"\"\"\n",
        "        sem_mask: (H,W) long semantic labels.\n",
        "\n",
        "        Returns:\n",
        "            binary_mask: (H,W) float {0,1}\n",
        "            point_coords: (1,2) float [[x, y]]\n",
        "            point_labels: (1,) long [1]\n",
        "        or None if we fail to sample.\n",
        "        \"\"\"\n",
        "        # Remove background / ignore labels as needed.\n",
        "        classes = torch.unique(sem_mask)\n",
        "        classes = classes[(classes != 0) & (classes != 255)]\n",
        "        if len(classes) == 0:\n",
        "            return None\n",
        "\n",
        "        cls = classes[torch.randint(len(classes), (1,))]\n",
        "        region = (sem_mask == cls)\n",
        "        ys, xs = region.nonzero(as_tuple=True)\n",
        "        if len(xs) == 0:\n",
        "            return None\n",
        "\n",
        "        idx = torch.randint(len(xs), (1,))\n",
        "        y, x = ys[idx], xs[idx]\n",
        "\n",
        "        binary_mask = region.float()\n",
        "        point_coords = torch.tensor([[float(x), float(y)]], dtype=torch.float32)\n",
        "        point_labels = torch.tensor([1], dtype=torch.int64)\n",
        "        return binary_mask, point_coords, point_labels\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        # DataLoader/Sampler sometimes passes numpy/torch scalar types; normalize them.\n",
        "        if not isinstance(idx, int):\n",
        "            idx = int(idx)\n",
        "        img, sem = self.base[idx]\n",
        "\n",
        "        # Force image -> RGB PIL (then ToTensor works reliably)\n",
        "        img = img.convert(\"RGB\")\n",
        "        img_t = T.ToTensor()(img)  # (3,H,W) float [0,1]\n",
        "\n",
        "        # Force semantic mask -> numpy int64 -> torch long\n",
        "        import numpy as np\n",
        "        sem_np = np.array(sem, dtype=np.int64)  # (H,W)\n",
        "        sem_t = torch.from_numpy(sem_np)        # long tensor\n",
        "\n",
        "        sample = self._sample_prompt_from_mask(sem_t)\n",
        "        if sample is None:\n",
        "            return self[random.randrange(len(self))]\n",
        "\n",
        "        binary_mask, point_coords, point_labels = sample\n",
        "        return {\n",
        "            \"image\": img_t,\n",
        "            \"gt_mask\": binary_mask,\n",
        "            \"point_coords\": point_coords,\n",
        "            \"point_labels\": point_labels,\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "def cityscapes_collate_fn(batch: list[Dict[str, Any]]) -> Dict[str, Tensor]:\n",
        "    \"\"\"\n",
        "    Simple collate for SAM-style training.\n",
        "    Assumes all images in batch have same size (true for Cityscapes).\n",
        "    \"\"\"\n",
        "    images = torch.stack([b[\"image\"] for b in batch], dim=0)             # (B,3,H,W)\n",
        "    gt_masks = torch.stack([b[\"gt_mask\"] for b in batch], dim=0)         # (B,H,W)\n",
        "    point_coords = torch.stack([b[\"point_coords\"] for b in batch], dim=0)  # (B,1,2)\n",
        "    point_labels = torch.stack([b[\"point_labels\"] for b in batch], dim=0)  # (B,1)\n",
        "\n",
        "    return {\n",
        "        \"image\": images,\n",
        "        \"gt_mask\": gt_masks,\n",
        "        \"point_coords\": point_coords,\n",
        "        \"point_labels\": point_labels,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cityscapes_for_sam import CityscapesForSAM\n",
        "\n",
        "ds = CityscapesForSAM(\"/content/drive/MyDrive/datasets\", split=\"train\")\n",
        "\n",
        "print(\"len:\", len(ds))\n",
        "x = ds[0]\n",
        "print(\"ok keys:\", x.keys())\n",
        "print(\"image:\", x[\"image\"].shape, x[\"image\"].dtype)\n",
        "print(\"gt_mask:\", x[\"gt_mask\"].shape, x[\"gt_mask\"].dtype)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnJQbJQ6tRWi",
        "outputId": "74be14f9-b4ec-4668-b98f-3414f86d3ac8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len: 2975\n",
            "ok keys: dict_keys(['image', 'gt_mask', 'point_coords', 'point_labels'])\n",
            "image: torch.Size([3, 1024, 2048]) torch.float32\n",
            "gt_mask: torch.Size([1024, 2048]) torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "print(os.listdir(\".\")[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lcPqlrngAF6",
        "outputId": "e9b51e6d-8f2e-4af5-e4ed-0c3b1f696ec8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "['.config', 'efficientvit', 'drive', 'assets', '__pycache__', 'cityscapes_for_sam.py', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Losses"
      ],
      "metadata": {
        "id": "V1sGVF_18LL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile losses_sam.py\n",
        "# losses_sam.py\n",
        "\"\"\"\n",
        "Loss functions for SAM-style training:\n",
        "- binary dice loss\n",
        "- binary focal loss\n",
        "- combined loss for multiple masks with best-of-N strategy.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def dice_loss(pred_probs: Tensor, target: Tensor, eps: float = 1e-6) -> Tensor:\n",
        "    \"\"\"\n",
        "    pred_probs, target: (B,1,H,W) in [0,1]\n",
        "    \"\"\"\n",
        "    intersection = (pred_probs * target).sum(dim=(2, 3))\n",
        "    union = pred_probs.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
        "    dice = (2.0 * intersection + eps) / (union + eps)\n",
        "    return 1.0 - dice.mean()\n",
        "\n",
        "\n",
        "def focal_loss(\n",
        "    pred_logits: Tensor,\n",
        "    target: Tensor,\n",
        "    alpha: float = 0.25,\n",
        "    gamma: float = 2.0,\n",
        "    eps: float = 1e-6,\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    pred_logits: (B,1,H,W) raw logits\n",
        "    target: (B,1,H,W) in {0,1}\n",
        "    \"\"\"\n",
        "    prob = torch.sigmoid(pred_logits)\n",
        "    pt = prob * target + (1 - prob) * (1 - target)\n",
        "    w = alpha * target + (1 - alpha) * (1 - target)\n",
        "    loss = -w * (1 - pt) ** gamma * torch.log(pt + eps)\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "def sam_mask_loss(multi_mask_logits: Tensor, gt_mask: Tensor) -> Tensor:\n",
        "    \"\"\"\n",
        "    Compute SAM-style loss given multiple masks (e.g., 3) and a single GT mask.\n",
        "\n",
        "    Args:\n",
        "        multi_mask_logits: (B, M, H, W) predicted logits for M masks\n",
        "        gt_mask:           (B, H, W)     float {0,1}\n",
        "\n",
        "    Returns:\n",
        "        Scalar loss = mean over batch of:\n",
        "            min_m [ 20 * focal_loss + 1 * dice_loss ]\n",
        "    \"\"\"\n",
        "    B, M, H, W = multi_mask_logits.shape\n",
        "\n",
        "    # Expand gt to match masks.\n",
        "    gt = gt_mask.unsqueeze(1).expand(-1, M, -1, -1)  # (B,M,H,W)\n",
        "\n",
        "    # Compute mask-wise losses.\n",
        "    logits_flat = multi_mask_logits.view(B * M, 1, H, W)\n",
        "    gt_flat = gt.view(B * M, 1, H, W)\n",
        "\n",
        "    fl = focal_loss(logits_flat, gt_flat)  # scalar over all (B*M) if we do it this way\n",
        "\n",
        "    # If you want **strict** per-mask best-of-N, uncomment this more detailed version:\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    #     prob_flat = torch.sigmoid(logits_flat)\n",
        "    # dl_per = dice_loss(prob_flat, gt_flat)  # currently scalar; reimplement to get per-sample if desired\n",
        "\n",
        "    # For a skeleton we keep it simple and just combine globally:\n",
        "    prob_flat = torch.sigmoid(logits_flat)\n",
        "    dl = dice_loss(prob_flat, gt_flat)\n",
        "\n",
        "    total = 20.0 * fl + 1.0 * dl\n",
        "    return total\n",
        "\n",
        "    # NOTE: If you want exact SAM behavior, modify this function to:\n",
        "    #   - compute focal+dice per sample & per mask (shape (B,M))\n",
        "    #   - take min over M for each sample\n",
        "    #   - average over B\n"
      ],
      "metadata": {
        "id": "YqImY7T90dO5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2f04bb9-10d9-4b6d-c51b-0805f6308bb9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing losses_sam.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training helper"
      ],
      "metadata": {
        "id": "_Vyzk6dA8Oev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#new version which bypasses predictors nograd\n",
        "%%writefile sam_forward_train.py\n",
        "# sam_forward_train.py\n",
        "from __future__ import annotations\n",
        "from typing import Dict\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from efficientvit.models.efficientvit.sam import EfficientViTSamPredictor\n",
        "\n",
        "def sam_forward_train(\n",
        "    predictor: EfficientViTSamPredictor,\n",
        "    batch: Dict[str, Tensor],\n",
        "    multimask_output: bool = True,\n",
        ") -> Tensor:\n",
        "    device = predictor.device\n",
        "    model = predictor.model\n",
        "\n",
        "    images: Tensor = batch[\"image\"].to(device)              # (B,3,H,W)\n",
        "    point_coords: Tensor = batch[\"point_coords\"].to(device) # (B,1,2)\n",
        "    point_labels: Tensor = batch[\"point_labels\"].to(device) # (B,1)\n",
        "\n",
        "    B, _, H, W = images.shape\n",
        "\n",
        "    # --- image encoder (frozen, but we still need embeddings) ---\n",
        "    # If image_encoder is frozen, no grads needed here; keeps memory low.\n",
        "    with torch.no_grad():\n",
        "        image_embeddings = model.image_encoder(images)\n",
        "\n",
        "    # --- prompt encoder (trainable if you left it requires_grad=True) ---\n",
        "    sparse_embeddings, dense_embeddings = model.prompt_encoder(\n",
        "        points=(point_coords, point_labels),\n",
        "        boxes=None,\n",
        "        masks=None,\n",
        "    )\n",
        "\n",
        "    # --- mask decoder (trainable head) ---\n",
        "    low_res_masks, iou_predictions = model.mask_decoder(\n",
        "        image_embeddings=image_embeddings,\n",
        "        image_pe=model.prompt_encoder.get_dense_pe(),\n",
        "        sparse_prompt_embeddings=sparse_embeddings,\n",
        "        dense_prompt_embeddings=dense_embeddings,\n",
        "        multimask_output=multimask_output,\n",
        "    )\n",
        "\n",
        "    # low_res_masks is usually (B, M, 256, 256)\n",
        "    # Upsample to full image size to match gt_mask\n",
        "    masks = torch.nn.functional.interpolate(\n",
        "        low_res_masks,\n",
        "        size=(H, W),\n",
        "        mode=\"bilinear\",\n",
        "        align_corners=False,\n",
        "    )\n",
        "    return masks  # (B, M, H, W) logits\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pr5VdreOcbh",
        "outputId": "ba643dfc-8b86-4562-f78b-2a20d9f0879f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sam_forward_train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Main training function"
      ],
      "metadata": {
        "id": "IB34vfPZ8UVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile train_efficientvit_sam_cityscapes.py\n",
        "# train_efficientvit_sam_cityscapes.py\n",
        "\"\"\"\n",
        "Head-only fine-tuning of EfficientViT-SAM on Cityscapes.\n",
        "\n",
        "Reuses:\n",
        "    - EfficientViTSam model\n",
        "    - EfficientViTSamPredictor for preprocessing & inference\n",
        "and only adds:\n",
        "    - Cityscapes dataset wrapper\n",
        "    - training loop\n",
        "    - SAM loss\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "from typing import Dict, Any\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from cityscapes_for_sam import CityscapesForSAM, cityscapes_collate_fn\n",
        "from losses_sam import sam_mask_loss\n",
        "from sam_forward_train import sam_forward_train  # or local function\n",
        "\n",
        "# Adjust to your repo entry point\n",
        "from efficientvit.sam_model_zoo import create_efficientvit_sam_model\n",
        "from efficientvit.models.efficientvit.sam import EfficientViTSamPredictor\n",
        "\n",
        "\n",
        "def build_model_and_predictor(\n",
        "    device: str,\n",
        "    model_name: str = \"efficientvit-sam-l0\",\n",
        "    weights_path: str | None = None,\n",
        ") -> tuple[nn.Module, EfficientViTSamPredictor]:\n",
        "    \"\"\"\n",
        "    Build EfficientViT-SAM model + predictor and set requires_grad flags.\n",
        "    \"\"\"\n",
        "    # Example; adjust args to match repo\n",
        "    model = create_efficientvit_sam_model(name=model_name, pretrained=True)\n",
        "    model.to(device)\n",
        "\n",
        "    # Freeze image encoder\n",
        "    for p in model.image_encoder.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Unfreeze mask decoder (head)\n",
        "    for p in model.mask_decoder.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    # Optionally also train prompt encoder\n",
        "    for p in model.prompt_encoder.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    predictor = EfficientViTSamPredictor(model)\n",
        "    return model, predictor\n",
        "\n",
        "\n",
        "def train(\n",
        "    cityscapes_root: str,\n",
        "    output_path: str = \"efficientvit_sam_head_finetuned_cityscapes.pt\",\n",
        "    model_name: str = \"efficientvit-sam-l0\",\n",
        "    batch_size: int = 2,\n",
        "    num_epochs: int = 10,\n",
        "    lr: float = 1e-4,\n",
        "    num_workers: int = 4,\n",
        ") -> None:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Datasets & loaders\n",
        "    train_ds = CityscapesForSAM(cityscapes_root, split=\"train\")\n",
        "    val_ds = CityscapesForSAM(cityscapes_root, split=\"val\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=cityscapes_collate_fn,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=cityscapes_collate_fn,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    # Model + predictor\n",
        "    model, predictor = build_model_and_predictor(\n",
        "        device=device,\n",
        "        model_name=model_name,\n",
        "        weights_path=None,  # or a checkpoint path if needed\n",
        "    )\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        [p for p in model.parameters() if p.requires_grad],\n",
        "        lr=lr,\n",
        "        weight_decay=1e-2,\n",
        "    )\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # ----------------- Train -----------------\n",
        "        model.train()\n",
        "        train_loss_accum = 0.0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [train]\"):\n",
        "            # Move GT to device\n",
        "            batch = {k: v.to(device) if isinstance(v, Tensor) else v for k, v in batch.items()}\n",
        "\n",
        "            # Forward through SAM predictor (you must implement sam_forward_train)\n",
        "            multi_mask_logits = sam_forward_train(predictor, batch)  # (B,M,H,W)\n",
        "\n",
        "            loss = sam_mask_loss(multi_mask_logits, batch[\"gt_mask\"])\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_accum += loss.item()\n",
        "\n",
        "        train_loss = train_loss_accum / max(1, len(train_loader))\n",
        "        print(f\"[Epoch {epoch+1}] train loss: {train_loss:.4f}\")\n",
        "\n",
        "        # ----------------- Validation -----------------\n",
        "        model.eval()\n",
        "        val_loss_accum = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [val]\"):\n",
        "                batch = {k: v.to(device) if isinstance(v, Tensor) else v for k, v in batch.items()}\n",
        "                multi_mask_logits = sam_forward_train(predictor, batch)\n",
        "                loss = sam_mask_loss(multi_mask_logits, batch[\"gt_mask\"])\n",
        "                val_loss_accum += loss.item()\n",
        "\n",
        "        val_loss = val_loss_accum / max(1, len(val_loader))\n",
        "        print(f\"[Epoch {epoch+1}] val loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), output_path)\n",
        "            print(f\"  -> Saved new best model to {output_path}\")\n",
        "\n",
        "    print(f\"Training complete. Best val loss = {best_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # CHANGE THIS to your Cityscapes root directory.\n",
        "    cityscapes_root = \"/content/drive/MyDrive/datasets\"\n",
        "    train(cityscapes_root, batch_size=4, num_workers=0)\n"
      ],
      "metadata": {
        "id": "3FUTNoC10iJO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "0dbc609c-b6e0-4d6d-b5c7-4c112e3389f2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [train]:   0%|          | 0/744 [00:03<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (16) must match the size of tensor b (4) at non-singleton dimension 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2109497484.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# CHANGE THIS to your Cityscapes root directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mcityscapes_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/datasets\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcityscapes_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2109497484.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cityscapes_root, output_path, model_name, batch_size, num_epochs, lr, num_workers)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# Forward through SAM predictor (you must implement sam_forward_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mmulti_mask_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msam_forward_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B,M,H,W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msam_mask_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_mask_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gt_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/sam_forward_train.py\u001b[0m in \u001b[0;36msam_forward_train\u001b[0;34m(predictor, batch, multimask_output)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# --- mask decoder (trainable head) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     low_res_masks, iou_predictions = model.mask_decoder(\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mimage_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mimage_pe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dense_pe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/segment_anything/modeling/mask_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings, multimask_output)\u001b[0m\n\u001b[1;32m     92\u001b[0m           \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatched\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmask\u001b[0m \u001b[0mquality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m         masks, iou_pred = self.predict_masks(\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mimage_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mimage_pe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_pe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/segment_anything/modeling/mask_decoder.py\u001b[0m in \u001b[0;36mpredict_masks\u001b[0;34m(self, image_embeddings, image_pe, sparse_prompt_embeddings, dense_prompt_embeddings)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# Expand per-image data in batch direction to be per-mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdense_prompt_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mpos_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_pe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (16) must match the size of tensor b (4) at non-singleton dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_mask_logits = sam_forward_train(predictor, batch)\n",
        "print(multi_mask_logits.requires_grad)"
      ],
      "metadata": {
        "id": "kOEmpV2sNSEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "SLYETJz0COGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import efficientvit.sam_model_zoo as zoo\n",
        "print([name for name in dir(zoo) if \"sam\" in name.lower() or \"model\" in name.lower()])\n"
      ],
      "metadata": {
        "id": "SftI_4GelIY6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}